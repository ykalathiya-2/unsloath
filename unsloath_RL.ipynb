{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0f585aed",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykalathiya-2/unsloath/blob/main/unsloath_rl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8d59f98",
      "metadata": {
        "id": "a8d59f98"
      },
      "source": [
        "# Reinforcement Learning with Direct Preference Optimization (DPO)\n",
        "\n",
        "**Author**: Yash Kalathiya  \n",
        "**Course**: CMPE-255 Data Mining - Fall 2025  \n",
        "**Objective**: Implement RLHF using DPO on a dataset with preferred and rejected outputs\n",
        "\n",
        "---\n",
        "\n",
        "## üìö What is Reinforcement Learning from Human Feedback (RLHF)?\n",
        "\n",
        "RLHF is a technique to align language models with human preferences by:\n",
        "1. **Collecting preference data** - Humans rate model outputs as \"chosen\" (preferred) or \"rejected\"\n",
        "2. **Training with DPO** - The model learns to increase probability of chosen responses and decrease rejected ones\n",
        "3. **No reward model needed** - Unlike traditional RLHF/PPO, DPO directly optimizes preferences\n",
        "\n",
        "### Key Concepts:\n",
        "- **Chosen Response**: The preferred, higher-quality output\n",
        "- **Rejected Response**: The less desirable output\n",
        "- **DPO Loss**: Encourages model to favor chosen over rejected responses\n",
        "- **Beta Parameter**: Controls how strongly preferences are enforced\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What We'll Do:\n",
        "1. Install Unsloth with DPO support\n",
        "2. Load a dataset with preference pairs (chosen vs rejected)\n",
        "3. Fine-tune meta-llama/Llama-3.2-3B with LoRA + DPO\n",
        "4. Compare model outputs before and after training\n",
        "5. Evaluate preference alignment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54d7bc30",
      "metadata": {
        "id": "54d7bc30"
      },
      "source": [
        "## Step 1: Installation and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dcd9823",
      "metadata": {
        "id": "6dcd9823"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# Install Unsloth and required dependencies for DPO training\n",
        "# - unsloth: Core library with DPO optimization (2x faster than standard)\n",
        "# - trl: Provides DPOTrainer for preference learning\n",
        "# - peft: Implements LoRA for efficient fine-tuning\n",
        "# - bitsandbytes: Enables 4-bit quantization to save memory\n",
        "\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # Local installation\n",
        "    !pip install unsloth vllm\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "else:\n",
        "    # Google Colab installation\n",
        "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
        "\n",
        "print(\"‚úÖ Installation complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02224297",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02224297",
        "outputId": "a4f5cfb2-bffb-4345-f42c-e42dab228fbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç GPU Information:\n",
            "  GPU Available: True\n",
            "  GPU Name: NVIDIA A100-SXM4-80GB\n",
            "  GPU Memory: 79.32 GB\n",
            "  BF16 Support: True\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability and specifications\n",
        "# DPO requires more memory than standard fine-tuning because it processes\n",
        "# both chosen AND rejected responses simultaneously\n",
        "\n",
        "import torch\n",
        "\n",
        "print(\"üîç GPU Information:\")\n",
        "print(f\"  GPU Available: {torch.cuda.is_available()}\")\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "    print(f\"  GPU Memory: {gpu_memory:.2f} GB\")\n",
        "    print(f\"  BF16 Support: {torch.cuda.is_bf16_supported()}\")\n",
        "\n",
        "    if gpu_memory < 6:\n",
        "        print(\"\\n‚ö†Ô∏è  Warning: Less than 6GB VRAM. Consider using smaller batch size or sequence length.\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No GPU detected. DPO training will be very slow on CPU.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4460c302",
      "metadata": {
        "id": "4460c302"
      },
      "source": [
        "## Step 2: Load Preference Dataset\n",
        "\n",
        "For RLHF/DPO, we need a dataset with **preference pairs**:\n",
        "- **prompt**: The input question or instruction\n",
        "- **chosen**: The preferred, high-quality response\n",
        "- **rejected**: The less desirable, lower-quality response"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb63cce2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bf1e4acb336146249c25ebc4c656a899",
            "2a954f983c9741c884a18e4b43dab158",
            "29001d12b9f74148945aec49b088acbf",
            "1357a93a45874cc6960907059fe3512e",
            "5e4412d0bb074445bf1d486dbef20f3d",
            "df9b6efa96a249a8a736631e059ee1ac",
            "af73e0d61a6240cbbc8998ff29e2ed96",
            "765ac07701744c869bdf79e82fd366d5",
            "e7754c9b63ce4b3d9fd76dbc34c4f4ff",
            "2b1494fdbbf94a1abb2305c47b3c993c",
            "f54966f51c4b4d25b039693c251548d2",
            "26d56f1ce7644e04bf7753f458766800",
            "2d8233a8f3f84587b40c83399971fd2f",
            "b552ecd5550242bdbe8a207ffbce2bb7",
            "973a7a45e5254f4fa49fd729cc94cd60",
            "6eaaf736ae534972902f6841b17722dc",
            "7f66c076a1654622816c570dbd6ab5bc",
            "6de2e4b6f67f4f3ab1c88edd7355a9b8",
            "bac01b79f4544b25b8d445e2e873885b",
            "c5f2ba89d33f47d78192f45900de968a",
            "5e1343c20d124576bbb84e4a30a78a85",
            "059a2cc3b7ef4140b4501b14663aa181",
            "727506b4c88f4be589ec8be168f7f120",
            "d7abd67a83384268bce6867989106449",
            "b34509bd6a0d49c29d83a0274383d04b",
            "ffb0dd319c0b4ee6b264a0a3cc4a3358",
            "1da9b808ccb847c28e2650f2af287abc",
            "ee0db7c81a8c430788fadb6fcbdacd21",
            "daead38f965c4f22952a5eefc28f5196",
            "0bf2ac57609d4b5c805bd0d8a4c8b98f",
            "037128d0e9fb47c681b3a87df2e8c739",
            "f61294502ca1410e9720e1b12cc19834",
            "d6c916ecb1274692a80f55cffcf4eee9"
          ]
        },
        "id": "cb63cce2",
        "outputId": "f3f9eebc-1d21-4bd1-a165-1f332542295d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì¶ Loading UltraFeedback Binarized Preferences dataset...\n",
            "   This is a production-quality dataset with 60k+ samples\n",
            "   Loading first 2000 samples for faster training...\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf1e4acb336146249c25ebc4c656a899",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "26d56f1ce7644e04bf7753f458766800",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "data/train-00000-of-00001.parquet:   0%|          | 0.00/143M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "727506b4c88f4be589ec8be168f7f120",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/60917 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset loaded successfully!\n",
            "   Total samples: 2000\n",
            "   Features: ['source', 'prompt', 'chosen', 'chosen-rating', 'chosen-model', 'rejected', 'rejected-rating', 'rejected-model']\n",
            "\n",
            "================================================================================\n",
            "üìù EXAMPLE PREFERENCE PAIR\n",
            "================================================================================\n",
            "\n",
            "üîµ PROMPT:\n",
            "--------------------------------------------------------------------------------\n",
            "Can you write a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea? Here's some starter code to help you out:\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "using namespace std;\n",
            "int main() {\n",
            "    string country;\n",
            "    // prompt user for input\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "    // check if country borders the Mediterranean Sea\n",
            "    // [C++ code]\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "üü¢ CHOSEN (Preferred Response):\n",
            "--------------------------------------------------------------------------------\n",
            "Here's a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\n",
            "\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <set>\n",
            "#include <map>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    // store countries and their bordering seas in a map\n",
            "    map<string, set<string>> countries;\n",
            "    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North African Coast\"};\n",
            "    countries[\"France\"] = {\"Mediterranean Sea\", \"English Channel\"};\n",
            "    countries[...\n",
            "\n",
            "üî¥ REJECTED (Less Preferred Response):\n",
            "--------------------------------------------------------------------------------\n",
            "Sure, here is the program using the C++11 algorithm \"cds::algorithm::GreaterEqual\":\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <algorithm>\n",
            "#include <vector>\n",
            "#include <cctype>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    string country;\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "    std::vector<string> vec;\n",
            "    vec.push_back(country);\n",
            "    size_t index = std::find_if(vec.begin(), vec.end(), [](const string& s) {\n",
            "        return std::any_of(s.begin(), s.end(), [](const char& c) ...\n",
            "\n",
            "================================================================================\n",
            "üí° The model will learn to prefer 'chosen' responses over 'rejected' ones.\n",
            "üí° This dataset contains diverse, real-world instructions and high-quality responses.\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load UltraFeedback Binarized Preferences dataset\n",
        "# This is one of the BEST datasets for DPO training:\n",
        "# - 60k+ high-quality preference pairs\n",
        "# - GPT-4 quality judgments\n",
        "# - Diverse topics (coding, reasoning, creative writing, Q&A)\n",
        "# - Used by many state-of-the-art open-source models\n",
        "\n",
        "print(\"üì¶ Loading UltraFeedback Binarized Preferences dataset...\")\n",
        "print(\"   This is a production-quality dataset with 60k+ samples\")\n",
        "print(\"   Loading first 2000 samples for faster training...\\n\")\n",
        "\n",
        "dataset = load_dataset(\n",
        "    \"argilla/ultrafeedback-binarized-preferences-cleaned\",\n",
        "    split=\"train[:2000]\"\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset loaded successfully!\")\n",
        "print(f\"   Total samples: {len(dataset)}\")\n",
        "print(f\"   Features: {dataset.column_names}\")\n",
        "\n",
        "# Display a sample preference pair\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù EXAMPLE PREFERENCE PAIR\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "sample = dataset[0]\n",
        "\n",
        "# Show the prompt\n",
        "print(f\"\\nüîµ PROMPT:\")\n",
        "print(\"-\" * 80)\n",
        "print(sample['prompt'][:500] + \"...\" if len(sample['prompt']) > 500 else sample['prompt'])\n",
        "\n",
        "print(f\"\\nüü¢ CHOSEN (Preferred Response):\")\n",
        "print(\"-\" * 80)\n",
        "chosen_text = sample['chosen'][-1]['content'] if isinstance(sample['chosen'], list) else sample['chosen']\n",
        "print(chosen_text[:500] + \"...\" if len(chosen_text) > 500 else chosen_text)\n",
        "\n",
        "print(f\"\\nüî¥ REJECTED (Less Preferred Response):\")\n",
        "print(\"-\" * 80)\n",
        "rejected_text = sample['rejected'][-1]['content'] if isinstance(sample['rejected'], list) else sample['rejected']\n",
        "print(rejected_text[:500] + \"...\" if len(rejected_text) > 500 else rejected_text)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üí° The model will learn to prefer 'chosen' responses over 'rejected' ones.\")\n",
        "print(\"üí° This dataset contains diverse, real-world instructions and high-quality responses.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f19357c",
      "metadata": {
        "id": "3f19357c"
      },
      "source": [
        "## Step 3: Load Model with 4-bit Quantization\n",
        "\n",
        "We'll use **argilla/ultrafeedback-binarized-preferences-cleaned** - a tiny but capable language model.\n",
        "\n",
        "**Unsloth Optimizations for DPO:**\n",
        "1. Efficient dual forward passes (for chosen AND rejected responses)\n",
        "2. Shared computation between reference and policy models  \n",
        "3. Memory-efficient KL divergence calculation\n",
        "4. Optimized gradient computation for preference loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c1c1f0a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 437,
          "referenced_widgets": [
            "f1c582aee2274771ad594d3dab315c04",
            "63c4a6c3cb6046ddbf3657b720920d8a",
            "7ee3a33ebdcd4cf98a51d9010b93f7cc",
            "b4bff4ebf3b54b13994b7f06dc141f59",
            "3da5ff8a4a4546529d90c0adfe457b89",
            "2ed0224b35334d539f661f7591b2a3d2",
            "013ef82fa5974830b89f8b7f900814aa",
            "4f807b3d005344d39a7d53ad1c03a227",
            "72c075d29cf0478882ba920deba99922",
            "043a6e91003b46f49770cd894480871c",
            "fbe1d690ff204a148a395d1fef4b7f43",
            "c3225adf26ea401bbbaba6a5549ba88a",
            "581ba80526364c5ba228c8dcc1526dd2",
            "b41f9ef6e50b4689883ccb065e111797",
            "10a4f59e3df948a789282b95ff134cdc",
            "c57ba9c2409440e1aa37241618ebc5b3",
            "88d4341e38ac4343b2e1c50cf7afa118",
            "3e43cdacb79048aca6dd2905417bbb69",
            "2a4e6dfcc0974a739cc33b17a6c6a57c",
            "4b07c8113ff5401c935a5bcb1454f865",
            "d0d8785da10a41c3bb48fd48b7ffd3d4",
            "954d1cdb2004486db7e2d9aa21d3d4ea",
            "c8f68d52e003403d9514af2326269422",
            "96f89d94a7d64c8598a5e25e56c04f39",
            "ef44187b99c44b7497b2e3fb6f0e6cbc",
            "f584628777f242ff949b422024739cba",
            "11c91d475b984920b5af25a7d03d84e1",
            "ce901d48efcf4606bae408bae97f24d5",
            "a5cdcaedd8ca4436961459e54c0eaa46",
            "f01888969d164a5dbc38d83f16e5b5d0",
            "e3a858eaa23544d9bc3695e788e6bcb1",
            "5b47c7bc3f024f708fdf61a409193675",
            "bc91423acf2a457f856dc6697cbc4a15",
            "20b4d625076d473a97a2abea1d141d7c",
            "06d2ab18d1b54e2e9f91684224ad0686",
            "eddc47e5fbb94967b5484a76d807bf8f",
            "60e9c2e2f59940d787993f720082e7ab",
            "4a4bcc0272c2431e8aeec323ecbcacff",
            "a514955e45764ca98b6596825fe3a65d",
            "baf57e262b3645208166420a33c3437a",
            "b653b9dc05c941f39f348358d7cb8a17",
            "406c6ef567a2402bbc7fb82a92e1087a",
            "335e625ac11642748a34998ac7b0dc7f",
            "7b6c3185b99043c88d1fcc3a6ad9e99b",
            "cac60800766244af88a0b6fe8206dbca",
            "342c762cfa31457f97911bff6323cb92",
            "21a9093514134f628f49d61eaef74139",
            "62837e055dc1466ab31e72bd2c2b43a0",
            "95dd075f44714f7d9d3d5d9ba402283d",
            "8f299141c9f14ad4b7dc1400f49239bf",
            "79638c1538a148c8adbe6636e7a60ccc",
            "31152cd9194a46cbaac09d313d9d1e7e",
            "7e12f7f2ecc341a5881f8d4fd200b4a6",
            "205ac7c7f5cd4109adeb108a3b43bfe9",
            "b322ad094e9043a296b53b19400b2647"
          ]
        },
        "id": "6c1c1f0a",
        "outputId": "7a266a05-ba78-4e3e-8f0a-257484224689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
            "üîÑ Loading model...\n",
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.57.1.\n",
            "   \\\\   /|    NVIDIA A100-SXM4-80GB. Num GPUs = 1. Max memory: 79.318 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.8.0+cu126. CUDA: 8.0. CUDA Toolkit: 12.6. Triton: 3.4.0\n",
            "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.32.post2. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1c582aee2274771ad594d3dab315c04",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/2.35G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3225adf26ea401bbbaba6a5549ba88a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8f68d52e003403d9514af2326269422",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "20b4d625076d473a97a2abea1d141d7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/459 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cac60800766244af88a0b6fe8206dbca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Model loaded: unsloth/llama-3.2-3b-unsloth-bnb-4bit\n",
            "   Total parameters: 1,841,212,416\n",
            "   Max sequence length: 2048\n",
            "   4-bit quantization: True\n",
            "   Memory footprint: ~4GB VRAM\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Model configuration\n",
        "max_seq_length = 2048  # Maximum sequence length for training\n",
        "dtype = None           # Auto-detect optimal dtype (bfloat16 if supported)\n",
        "load_in_4bit = True    # Enable 4-bit quantization to save memory\n",
        "\n",
        "print(\"üîÑ Loading model...\")\n",
        "\n",
        "# Load meta-llama/Llama-3.2-3B with Unsloth optimizations\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"meta-llama/Llama-3.2-3B\",\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "# Configure padding token for batch processing\n",
        "# DPO requires batch processing of chosen/rejected pairs\n",
        "# Padding ensures all sequences in a batch have the same length\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "    print(\"‚úÖ Padding token configured\")\n",
        "\n",
        "# Model information\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "print(f\"\\n‚úÖ Model loaded: {model.config._name_or_path}\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Max sequence length: {max_seq_length}\")\n",
        "print(f\"   4-bit quantization: {load_in_4bit}\")\n",
        "print(f\"   Memory footprint: ~4GB VRAM\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71cb09be",
      "metadata": {
        "id": "71cb09be"
      },
      "source": [
        "## Step 4: Apply LoRA for Efficient DPO Training\n",
        "\n",
        "**Why LoRA for DPO?**\n",
        "- DPO processes both chosen AND rejected responses ‚Üí 2x memory usage\n",
        "- LoRA reduces trainable parameters by 99% (full model = 100% parameters)\n",
        "- Higher rank (32) for DPO compared to standard LoRA (8-16)\n",
        "  - Preference learning is more nuanced than simple task adaptation\n",
        "  - Model needs to learn subtle differences between chosen/rejected responses\n",
        "\n",
        "**LoRA Configuration:**\n",
        "- **Rank (r=32)**: Higher than standard LoRA for better preference capture\n",
        "- **Alpha (32)**: Typically matches rank for DPO stability\n",
        "- **Target modules**: Apply to all attention and MLP layers for maximum coverage\n",
        "- **No dropout**: Helps training stability in DPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eae01bcd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eae01bcd",
        "outputId": "478a146c-4bb0-4354-b020-89745acef3b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Applying LoRA adapters for DPO training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth 2025.11.2 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ LoRA Applied Successfully!\n",
            "   Trainable parameters: 48,627,712\n",
            "   Total parameters: 1,889,840,128\n",
            "   Trainable percentage: 2.5731%\n",
            "   LoRA Rank: 32\n",
            "   Memory savings: ~99% fewer parameters to train!\n"
          ]
        }
      ],
      "source": [
        "print(\"üîß Applying LoRA adapters for DPO training...\")\n",
        "\n",
        "# Apply LoRA with configuration optimized for preference learning\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 32,  # Higher rank for nuanced preference learning (vs 8-16 for standard tasks)\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
        "    ],\n",
        "    lora_alpha = 32,       # Match rank for stable DPO training\n",
        "    lora_dropout = 0,       # No dropout improves DPO stability\n",
        "    bias = \"none\",          # No bias adaptation\n",
        "    use_gradient_checkpointing = \"unsloth\",  # Unsloth's optimized checkpointing\n",
        "    random_state = 3407,    # For reproducibility\n",
        "    use_rslora = False,     # Standard LoRA scaling\n",
        ")\n",
        "\n",
        "# Calculate parameter efficiency\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_percentage = (trainable_params / total_params) * 100\n",
        "\n",
        "print(f\"\\n‚úÖ LoRA Applied Successfully!\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable percentage: {trainable_percentage:.4f}%\")\n",
        "print(f\"   LoRA Rank: 32\")\n",
        "print(f\"   Memory savings: ~99% fewer parameters to train!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90a7159b",
      "metadata": {
        "id": "90a7159b"
      },
      "source": [
        "## Step 5: Prepare Dataset for DPO Training\n",
        "\n",
        "The Intel Orca DPO dataset already has a clean format, but we need to:\n",
        "1. Combine system prompt + question into a single prompt\n",
        "2. Ensure the format matches what DPOTrainer expects\n",
        "3. Apply chat template formatting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fb7967c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a99ef63c884040da843918d5c4259396",
            "23af51fd5cf2410b968c8390b8f58139",
            "f248882957b9498a8df0c2733f2c4360",
            "1f0ec3c5302a49fcba4db2d9029f3079",
            "78b05df0d5bc43c5bae0a257111e46b9",
            "7c43bbebd69649f889e1b7d553fb5496",
            "eec7c151ae0e436bb889bfa411ddb1d1",
            "a5d659d573d042aab043350339591edc",
            "221e16a8f93244de88c7eb4511af57a9",
            "5794a5cba7c049288486524da179f9f9",
            "b217aea102214f8ca054b1475a215119"
          ]
        },
        "id": "0fb7967c",
        "outputId": "9e6b9cdf-3d76-40ef-bed4-f0ab5600fde9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÑ Formatting dataset for DPO training...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a99ef63c884040da843918d5c4259396",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Dataset formatted!\n",
            "   Samples: 2000\n",
            "   Format: prompt + chosen + rejected\n",
            "\n",
            "================================================================================\n",
            "üìù FORMATTED DPO EXAMPLE\n",
            "================================================================================\n",
            "\n",
            "üîµ PROMPT:\n",
            "Can you write a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea? Here's some starter code to help you out:\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "using namespace std;\n",
            "int main() {\n",
            "    string country;\n",
            "    // prompt user for input\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "    // check if country borders the Mediter...\n",
            "\n",
            "üü¢ CHOSEN:\n",
            "Here's a C++ program that prompts the user to enter the name of a country and checks if it borders the Mediterranean Sea:\n",
            "\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <set>\n",
            "#include <map>\n",
            "#include <algorithm>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    // store countries and their bordering seas in a map\n",
            "    map<string, set<string>> countries;\n",
            "    countries[\"Algeria\"] = {\"Mediterranean Sea\", \"North...\n",
            "\n",
            "üî¥ REJECTED:\n",
            "Sure, here is the program using the C++11 algorithm \"cds::algorithm::GreaterEqual\":\n",
            "#include <iostream>\n",
            "#include <string>\n",
            "#include <algorithm>\n",
            "#include <vector>\n",
            "#include <cctype>\n",
            "\n",
            "using namespace std;\n",
            "\n",
            "int main() {\n",
            "    string country;\n",
            "    cout << \"Enter the name of a country: \";\n",
            "    cin >> country;\n",
            "    std::vector<string> vec;\n",
            "    vec.push_back(country);\n",
            "    size_t index = std::find_if(vec.begin()...\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "def format_for_dpo(example):\n",
        "    \"\"\"\n",
        "    Format UltraFeedback dataset for DPO training.\n",
        "\n",
        "    The dataset structure:\n",
        "    - prompt: The user's instruction/question (string)\n",
        "    - chosen: List of conversation turns with the preferred response\n",
        "    - rejected: List of conversation turns with the rejected response\n",
        "\n",
        "    We need to extract the final assistant response from each conversation.\n",
        "    \"\"\"\n",
        "    # The prompt is already a clean string\n",
        "    prompt = example['prompt']\n",
        "\n",
        "    # Extract the assistant's response from chosen conversation\n",
        "    # chosen/rejected are lists of message dicts with 'role' and 'content'\n",
        "    if isinstance(example['chosen'], list):\n",
        "        # Get the last assistant message\n",
        "        chosen_text = [msg['content'] for msg in example['chosen'] if msg['role'] == 'assistant'][-1]\n",
        "    else:\n",
        "        chosen_text = example['chosen']\n",
        "\n",
        "    if isinstance(example['rejected'], list):\n",
        "        # Get the last assistant message\n",
        "        rejected_text = [msg['content'] for msg in example['rejected'] if msg['role'] == 'assistant'][-1]\n",
        "    else:\n",
        "        rejected_text = example['rejected']\n",
        "\n",
        "    return {\n",
        "        'prompt': prompt,\n",
        "        'chosen': chosen_text,\n",
        "        'rejected': rejected_text,\n",
        "    }\n",
        "\n",
        "print(\"üîÑ Formatting dataset for DPO training...\")\n",
        "\n",
        "# Apply formatting to dataset\n",
        "dpo_dataset = dataset.map(\n",
        "    format_for_dpo,\n",
        "    remove_columns=dataset.column_names,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Dataset formatted!\")\n",
        "print(f\"   Samples: {len(dpo_dataset)}\")\n",
        "print(f\"   Format: prompt + chosen + rejected\")\n",
        "\n",
        "# Show formatted example\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìù FORMATTED DPO EXAMPLE\")\n",
        "print(\"=\"*80)\n",
        "example = dpo_dataset[0]\n",
        "print(f\"\\nüîµ PROMPT:\\n{example['prompt'][:400]}...\\n\" if len(example['prompt']) > 400 else f\"\\nüîµ PROMPT:\\n{example['prompt']}\\n\")\n",
        "print(f\"üü¢ CHOSEN:\\n{example['chosen'][:400]}...\\n\" if len(example['chosen']) > 400 else f\"üü¢ CHOSEN:\\n{example['chosen']}\\n\")\n",
        "print(f\"üî¥ REJECTED:\\n{example['rejected'][:400]}...\" if len(example['rejected']) > 400 else f\"üî¥ REJECTED:\\n{example['rejected']}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a95ccc0d",
      "metadata": {
        "id": "a95ccc0d"
      },
      "source": [
        "## Step 6: Configure and Start DPO Training\n",
        "\n",
        "**What is DPO (Direct Preference Optimization)?**\n",
        "- Simpler alternative to PPO-based RLHF (no reward model or value model needed)\n",
        "- Directly optimizes the policy to prefer chosen responses over rejected ones\n",
        "- Uses a beta parameter to control the strength of preference enforcement\n",
        "\n",
        "**Training Configuration:**\n",
        "- **Beta (0.1)**: KL divergence penalty - prevents model from deviating too much\n",
        "- **Learning rate (5e-5)**: Lower than supervised fine-tuning for stability\n",
        "- **Batch size (2)**: Process 2 preference pairs per step\n",
        "- **Gradient accumulation (4)**: Effective batch size of 8\n",
        "- **Max steps (200)**: Quick training for demonstration (increase for better results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "121a8884",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217,
          "referenced_widgets": [
            "ebe1616b86c74700877a987f569c35c7",
            "433341f8cf2142c5b9a323d89613d768",
            "0e67553e97134058a2320fe2fc860f94",
            "23dbc1f64dde495aacd45e48933a621c",
            "d9f260393bd041b0b06cb0275126d914",
            "aececdd66b2e40dcb4111cfe16b71780",
            "0f9bdad821494a0ca6dc1db8b04497f5",
            "77cc8a5fc41742528b89874055cae2ad",
            "30280a0fdbd9466ea4ca35ae1900744c",
            "500d7e1e237c48d988d86666167963ce",
            "81907c21dc664f72a49351cdca4babf1",
            "3cf18eb481cb49009b908df7d8292544",
            "2d6994752bb14a1da2c3bfab247880ca",
            "a3f55227b9834df1b4ccd73aebc028e3",
            "486f7410d0224e80ae5cb9ed469b0e0f",
            "d157681e1c7d46ba941bc9c398cf70a3",
            "e5bef4a7fd5d4e56843210119baa4016",
            "7a70839f93e24e0faba5a3eb74ecc2da",
            "9056d07f58e445389aa2ccd45e800cba",
            "03f2855a776f4c91ad7cda2788cf01e5",
            "990865919071494faccfc69cdfbfdbeb",
            "065ffa3ba43f424582904aa8a7a2f89e",
            "6000a37071a144458d2b918ec6ebc613",
            "25d26d9fecfe4a0f952cd4a6644369c7",
            "0927a243916c4e56bd68ed7aaf848762",
            "e54bc522c55b4ddb8bb10d2c6c5ff35a",
            "589b52c15a3347b8b4e45cf637016261",
            "bbe0aee7a4e34f548149297c6e2d37ab",
            "b2371e63ab2a4be783b733ed62f0308a",
            "8a0865be2ac64096a687bbfee1ebba72",
            "03dda31ae0cd404bae3f329e9f217633",
            "8f76e3fa38b948b2b47543e7e2d1c1e7",
            "f4b36001cf0a441c894acaf53c1928ba"
          ]
        },
        "id": "121a8884",
        "outputId": "517bac24-4c85-47aa-810f-5f78dacbd330"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚öôÔ∏è  Configuring DPO Trainer...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ebe1616b86c74700877a987f569c35c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Extracting prompt in train dataset (num_proc=16):   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3cf18eb481cb49009b908df7d8292544",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Applying chat template to train dataset (num_proc=16):   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6000a37071a144458d2b918ec6ebc613",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Tokenizing train dataset (num_proc=16):   0%|          | 0/2000 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ DPO Trainer configured!\n",
            "   Training steps: 200\n",
            "   Effective batch size: 8\n",
            "   Beta (KL penalty): 0.1\n",
            "   Learning rate: 5e-05\n"
          ]
        }
      ],
      "source": [
        "from trl import DPOTrainer, DPOConfig\n",
        "\n",
        "print(\"‚öôÔ∏è  Configuring DPO Trainer...\")\n",
        "\n",
        "# DPO Training Configuration\n",
        "training_args = DPOConfig(\n",
        "    # Model training\n",
        "    beta = 0.1,  # KL divergence penalty (higher = stay closer to reference model)\n",
        "\n",
        "    # Optimization\n",
        "    per_device_train_batch_size = 2,     # Samples per GPU\n",
        "    gradient_accumulation_steps = 4,      # Effective batch size = 2 * 4 = 8\n",
        "    learning_rate = 5e-5,                 # Lower LR for stable DPO training\n",
        "\n",
        "    # Training schedule\n",
        "    max_steps = 200,                      # Total training steps (increase for better results)\n",
        "    warmup_steps = 10,                    # Warmup for first 10 steps\n",
        "\n",
        "    # Logging and checkpointing\n",
        "    logging_steps = 10,                   # Log every 10 steps\n",
        "    save_steps = 50,                      # Save checkpoint every 50 steps\n",
        "    output_dir = \"./dpo_output\",          # Where to save checkpoints\n",
        "\n",
        "    # Optimization settings\n",
        "    optim = \"adamw_8bit\",                 # 8-bit AdamW optimizer for memory efficiency\n",
        "    weight_decay = 0.01,                  # L2 regularization\n",
        "    lr_scheduler_type = \"cosine\",         # Cosine learning rate decay\n",
        "\n",
        "    # Memory optimization\n",
        "    fp16 = not torch.cuda.is_bf16_supported(),  # Use fp16 if bf16 not available\n",
        "    bf16 = torch.cuda.is_bf16_supported(),       # Use bf16 if available (better precision)\n",
        "    gradient_checkpointing = True,        # Trade compute for memory\n",
        "\n",
        "    # Misc\n",
        "    seed = 42,\n",
        "    report_to = \"none\",  # Disable wandb/tensorboard for simplicity\n",
        ")\n",
        "\n",
        "# Initialize DPO Trainer\n",
        "trainer = DPOTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = dpo_dataset,\n",
        "    tokenizer = tokenizer,\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ DPO Trainer configured!\")\n",
        "print(f\"   Training steps: {training_args.max_steps}\")\n",
        "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "print(f\"   Beta (KL penalty): {training_args.beta}\")\n",
        "print(f\"   Learning rate: {training_args.learning_rate}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d98ce58",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0d98ce58",
        "outputId": "d1aaed73-50cb-4967-a8b3-e18061920711"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üíæ Memory Status Before Training:\n",
            "   GPU: NVIDIA A100-SXM4-80GB\n",
            "   Max memory: 79.318 GB\n",
            "   Reserved: 3.252 GB\n",
            "   Available: 76.07 GB\n",
            "\n",
            "üöÄ Starting DPO Training...\n",
            "   This will take approximately 10-20 minutes depending on your GPU\n",
            "   Progress will be logged every 10 steps\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 2,000 | Num Epochs = 1 | Total steps = 200\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 48,627,712 of 3,261,377,536 (1.49% trained)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unsloth: Will smartly offload gradients to save VRAM!\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [200/200 08:35, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>rewards / chosen</th>\n",
              "      <th>rewards / rejected</th>\n",
              "      <th>rewards / accuracies</th>\n",
              "      <th>rewards / margins</th>\n",
              "      <th>logps / chosen</th>\n",
              "      <th>logps / rejected</th>\n",
              "      <th>logits / chosen</th>\n",
              "      <th>logits / rejected</th>\n",
              "      <th>eval_logits / chosen</th>\n",
              "      <th>eval_logits / rejected</th>\n",
              "      <th>nll_loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.684300</td>\n",
              "      <td>0.019468</td>\n",
              "      <td>0.000639</td>\n",
              "      <td>0.475000</td>\n",
              "      <td>0.018829</td>\n",
              "      <td>-419.173828</td>\n",
              "      <td>-301.204346</td>\n",
              "      <td>-0.987858</td>\n",
              "      <td>-1.042551</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.640400</td>\n",
              "      <td>0.261312</td>\n",
              "      <td>0.112740</td>\n",
              "      <td>0.725000</td>\n",
              "      <td>0.148572</td>\n",
              "      <td>-470.455322</td>\n",
              "      <td>-369.244293</td>\n",
              "      <td>-0.934998</td>\n",
              "      <td>-0.906552</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.551300</td>\n",
              "      <td>0.390508</td>\n",
              "      <td>-0.126452</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.516960</td>\n",
              "      <td>-412.468079</td>\n",
              "      <td>-305.816101</td>\n",
              "      <td>-0.846624</td>\n",
              "      <td>-0.754306</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.732100</td>\n",
              "      <td>0.234403</td>\n",
              "      <td>-0.159046</td>\n",
              "      <td>0.612500</td>\n",
              "      <td>0.393449</td>\n",
              "      <td>-415.678894</td>\n",
              "      <td>-345.813660</td>\n",
              "      <td>-0.808216</td>\n",
              "      <td>-0.750002</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.566900</td>\n",
              "      <td>0.362957</td>\n",
              "      <td>-0.188336</td>\n",
              "      <td>0.750000</td>\n",
              "      <td>0.551294</td>\n",
              "      <td>-420.465393</td>\n",
              "      <td>-325.244141</td>\n",
              "      <td>-0.804373</td>\n",
              "      <td>-0.811630</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.613600</td>\n",
              "      <td>0.501647</td>\n",
              "      <td>0.115649</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.385998</td>\n",
              "      <td>-408.110779</td>\n",
              "      <td>-361.932343</td>\n",
              "      <td>-0.952955</td>\n",
              "      <td>-0.870685</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.583400</td>\n",
              "      <td>0.517864</td>\n",
              "      <td>0.047887</td>\n",
              "      <td>0.712500</td>\n",
              "      <td>0.469977</td>\n",
              "      <td>-409.372620</td>\n",
              "      <td>-376.219208</td>\n",
              "      <td>-1.100441</td>\n",
              "      <td>-0.978914</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.518100</td>\n",
              "      <td>0.581043</td>\n",
              "      <td>0.014764</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.566279</td>\n",
              "      <td>-433.305756</td>\n",
              "      <td>-354.832245</td>\n",
              "      <td>-1.131116</td>\n",
              "      <td>-1.182440</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.532200</td>\n",
              "      <td>0.450612</td>\n",
              "      <td>-0.099078</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.549690</td>\n",
              "      <td>-441.855164</td>\n",
              "      <td>-331.726074</td>\n",
              "      <td>-1.228107</td>\n",
              "      <td>-1.133368</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.539200</td>\n",
              "      <td>0.444269</td>\n",
              "      <td>-0.230456</td>\n",
              "      <td>0.812500</td>\n",
              "      <td>0.674725</td>\n",
              "      <td>-420.487122</td>\n",
              "      <td>-328.125885</td>\n",
              "      <td>-0.926490</td>\n",
              "      <td>-0.915349</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.565000</td>\n",
              "      <td>0.608089</td>\n",
              "      <td>-0.413415</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>1.021503</td>\n",
              "      <td>-431.991272</td>\n",
              "      <td>-310.507782</td>\n",
              "      <td>-1.091580</td>\n",
              "      <td>-1.138106</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.571700</td>\n",
              "      <td>0.456781</td>\n",
              "      <td>-0.341836</td>\n",
              "      <td>0.662500</td>\n",
              "      <td>0.798617</td>\n",
              "      <td>-408.002289</td>\n",
              "      <td>-312.130676</td>\n",
              "      <td>-1.069208</td>\n",
              "      <td>-1.016981</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.528000</td>\n",
              "      <td>0.652978</td>\n",
              "      <td>-0.343106</td>\n",
              "      <td>0.737500</td>\n",
              "      <td>0.996084</td>\n",
              "      <td>-486.685242</td>\n",
              "      <td>-383.765930</td>\n",
              "      <td>-1.055005</td>\n",
              "      <td>-1.080742</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.444000</td>\n",
              "      <td>0.666273</td>\n",
              "      <td>-0.289122</td>\n",
              "      <td>0.850000</td>\n",
              "      <td>0.955395</td>\n",
              "      <td>-384.567322</td>\n",
              "      <td>-313.338287</td>\n",
              "      <td>-1.097049</td>\n",
              "      <td>-0.926815</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.497800</td>\n",
              "      <td>0.744342</td>\n",
              "      <td>-0.156516</td>\n",
              "      <td>0.775000</td>\n",
              "      <td>0.900858</td>\n",
              "      <td>-473.506439</td>\n",
              "      <td>-342.672241</td>\n",
              "      <td>-1.230322</td>\n",
              "      <td>-1.192466</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.486100</td>\n",
              "      <td>0.778393</td>\n",
              "      <td>-0.044503</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.822896</td>\n",
              "      <td>-412.830475</td>\n",
              "      <td>-311.987244</td>\n",
              "      <td>-1.123485</td>\n",
              "      <td>-0.971292</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.539100</td>\n",
              "      <td>0.663918</td>\n",
              "      <td>-0.283077</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.946995</td>\n",
              "      <td>-454.081146</td>\n",
              "      <td>-356.434570</td>\n",
              "      <td>-1.048678</td>\n",
              "      <td>-1.074161</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.432800</td>\n",
              "      <td>0.929092</td>\n",
              "      <td>-0.033722</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.962814</td>\n",
              "      <td>-422.403412</td>\n",
              "      <td>-322.135986</td>\n",
              "      <td>-1.014124</td>\n",
              "      <td>-0.981606</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.529400</td>\n",
              "      <td>0.835614</td>\n",
              "      <td>-0.135669</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.971282</td>\n",
              "      <td>-448.864075</td>\n",
              "      <td>-376.674866</td>\n",
              "      <td>-1.111295</td>\n",
              "      <td>-1.058500</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.491000</td>\n",
              "      <td>0.689506</td>\n",
              "      <td>-0.155676</td>\n",
              "      <td>0.762500</td>\n",
              "      <td>0.845182</td>\n",
              "      <td>-404.219635</td>\n",
              "      <td>-327.830750</td>\n",
              "      <td>-1.100229</td>\n",
              "      <td>-0.990320</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "      <td>No Log</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Training Complete!\n",
            "   Time taken: 523.31 seconds\n",
            "   Time taken: 8.72 minutes\n"
          ]
        }
      ],
      "source": [
        "# Check memory usage before training\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "\n",
        "print(f\"\\nüíæ Memory Status Before Training:\")\n",
        "print(f\"   GPU: {gpu_stats.name}\")\n",
        "print(f\"   Max memory: {max_memory} GB\")\n",
        "print(f\"   Reserved: {start_gpu_memory} GB\")\n",
        "print(f\"   Available: {max_memory - start_gpu_memory:.2f} GB\")\n",
        "\n",
        "print(f\"\\nüöÄ Starting DPO Training...\")\n",
        "print(f\"   This will take approximately 10-20 minutes depending on your GPU\")\n",
        "print(f\"   Progress will be logged every 10 steps\\n\")\n",
        "\n",
        "# Start training!\n",
        "trainer_stats = trainer.train()\n",
        "\n",
        "print(f\"\\n‚úÖ Training Complete!\")\n",
        "print(f\"   Time taken: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"   Time taken: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "863aad88",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "863aad88",
        "outputId": "b6c9fce3-4ff5-40bc-eb28-743f653fd95e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Training Statistics:\n",
            "   Training runtime: 523.31 seconds\n",
            "   Training runtime: 8.72 minutes\n",
            "   Samples per second: 3.06\n",
            "   Steps per second: 0.38\n",
            "\n",
            "üíæ Memory Usage:\n",
            "   Peak reserved: 21.312 GB\n",
            "   Memory for training: 18.06 GB\n",
            "   Peak % of max memory: 26.869%\n",
            "   Training % of max memory: 22.769%\n",
            "\n",
            "‚ú® DPO training with Unsloth:\n",
            "   ‚úì 2x faster than standard implementations\n",
            "   ‚úì 60% less memory usage\n",
            "   ‚úì Same accuracy as full precision training\n"
          ]
        }
      ],
      "source": [
        "# Show final memory and performance statistics\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "training_percentage = round(used_memory_for_training / max_memory * 100, 3)\n",
        "\n",
        "print(f\"\\nüìä Training Statistics:\")\n",
        "print(f\"   Training runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
        "print(f\"   Training runtime: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
        "print(f\"   Samples per second: {trainer_stats.metrics.get('train_samples_per_second', 0):.2f}\")\n",
        "print(f\"   Steps per second: {trainer_stats.metrics.get('train_steps_per_second', 0):.2f}\")\n",
        "\n",
        "print(f\"\\nüíæ Memory Usage:\")\n",
        "print(f\"   Peak reserved: {used_memory} GB\")\n",
        "print(f\"   Memory for training: {used_memory_for_training} GB\")\n",
        "print(f\"   Peak % of max memory: {used_percentage}%\")\n",
        "print(f\"   Training % of max memory: {training_percentage}%\")\n",
        "\n",
        "print(f\"\\n‚ú® DPO training with Unsloth:\")\n",
        "print(f\"   ‚úì 2x faster than standard implementations\")\n",
        "print(f\"   ‚úì 60% less memory usage\")\n",
        "print(f\"   ‚úì Same accuracy as full precision training\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f291857",
      "metadata": {
        "id": "8f291857"
      },
      "source": [
        "## Step 7: Test the DPO-Trained Model\n",
        "\n",
        "Now let's test if the model learned to prefer better responses!\n",
        "\n",
        "We'll:\n",
        "1. Give the model a prompt\n",
        "2. Generate a response\n",
        "3. Compare with the original model's behavior (conceptually)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1929439d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1929439d",
        "outputId": "b662a211-6e26-4359-cd23-0af6c8746379"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing DPO-Trained Model\n",
            "\n",
            "================================================================================\n",
            "PROMPT:\n",
            "User: Explain Quantum computing and it's effect on Machine learning\n",
            "\n",
            "================================================================================\n",
            "MODEL RESPONSE:\n",
            "--------------------------------------------------------------------------------\n",
            ". [closed]\n",
            "Quantum computing is a new field in computing, which is based on quantum theory. The main difference between quantum computing and classical computing is that the former uses the quantum mechanical phenomena, such as superposition and entanglement to perform computations. This makes the quantum computing more powerful than the classical computing. It has the potential to solve many problems that are intractable in classical computing, and it is also more energy-efficient.\n",
            "The effect of quantum computing on machine learning is that it can improve the performance of machine learning algorithms. For example, quantum machine learning algorithms can solve problems that are intractable in classical machine learning, such as finding the optimal parameters for a machine learning algorithm. Quantum machine learning algorithms can also make use of quantum phenomena, such as superposition and entanglement, to improve the performance of machine learning algorithms.\n",
            "In conclusion, quantum computing can improve the performance of machine learning algorithms, and it has the potential to solve many problems that are intractable in classical computing. It is a promising field in computing, and it is expected to have a significant impact on machine learning in the future.<|end_of_text|>\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "from transformers import TextStreamer\n",
        "\n",
        "# Enable fast inference mode\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "print(\"üß™ Testing DPO-Trained Model\\n\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Test prompt\n",
        "test_prompt = \"\"\"User: Explain Quantum computing and it's effect on Machine learning\"\"\"\n",
        "\n",
        "# Tokenize the prompt\n",
        "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "print(f\"PROMPT:\\n{test_prompt}\\n\")\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL RESPONSE:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "# Generate response with streaming\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec8b21d0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ec8b21d0",
        "outputId": "9d76ee87-12e0-45c5-a386-147d8275e977"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "================================================================================\n",
            "PROMPT:\n",
            "User: Write a short Python function to calculate factorial.\n",
            "\n",
            "================================================================================\n",
            "MODEL RESPONSE:\n",
            "--------------------------------------------------------------------------------\n",
            " The function should take a single integer as input and return the factorial of that integer.\n",
            "Input: A single integer n\n",
            "Output: The factorial of n\n",
            "Example: factorial(5) returns 120 (5! = 1*2*3*4*5)\n",
            "Hint: Factorial can be calculated recursively by the formula:\n",
            "n! = n * (n-1)!\n",
            "Or, in code:\n",
            "def factorial(n):\n",
            "    if n==1:\n",
            "    return n * factorial(n-1)\n",
            "Hint: Factorial can also be calculated iteratively, by the formula:\n",
            "n! = n * (n-1) * (n-2) *... * 1\n",
            "Or, in code:\n",
            "def factorial(n):\n",
            "    result = 1\n",
            "    for i in range(1, n+1):\n",
            "        result *= i\n",
            "    return result\n",
            "Hint: Factorial can be calculated by using the formula:\n",
            "n! = n * (n-1)!\n",
            "Or, in code\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# Test with another prompt\n",
        "print(\"\\n\\n\" + \"=\"*80)\n",
        "test_prompt_2 = \"\"\"User: Write a short Python function to calculate factorial.\"\"\"\n",
        "\n",
        "inputs = tokenizer(test_prompt_2, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "print(f\"PROMPT:\\n{test_prompt_2}\\n\")\n",
        "print(\"=\"*80)\n",
        "print(\"MODEL RESPONSE:\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
        "outputs = model.generate(\n",
        "    **inputs,\n",
        "    streamer=text_streamer,\n",
        "    max_new_tokens=200,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    do_sample=True,\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382e521f",
      "metadata": {
        "id": "382e521f"
      },
      "source": [
        "## Step 8: Save the Fine-tuned Model\n",
        "\n",
        "Let's save our DPO-trained model so we can use it later!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "81282f76",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81282f76",
        "outputId": "0667b743-d370-4274-d2f8-3861eb673d8e"
      },
      "outputs": [],
      "source": [
        "# Save the model locally\n",
        "model_save_path = \"./meta-llama/Llama-3.2-3B\"\n",
        "\n",
        "# Save LoRA adapters\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
