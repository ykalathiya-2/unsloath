{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f9b5652",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ykalathiya-2/unsloath/blob/main/unsloath_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d59f98",
   "metadata": {
    "id": "a8d59f98"
   },
   "source": [
    "# Reinforcement Learning with Direct Preference Optimization (DPO)\n",
    "\n",
    "**Author**: Yash Kalathiya  \n",
    "**Course**: CMPE-255 Data Mining - Fall 2025  \n",
    "**Objective**: Implement RLHF using DPO on a dataset with preferred and rejected outputs\n",
    "\n",
    "---\n",
    "\n",
    "## üìö What is Reinforcement Learning from Human Feedback (RLHF)?\n",
    "\n",
    "RLHF is a technique to align language models with human preferences by:\n",
    "1. **Collecting preference data** - Humans rate model outputs as \"chosen\" (preferred) or \"rejected\"\n",
    "2. **Training with DPO** - The model learns to increase probability of chosen responses and decrease rejected ones\n",
    "3. **No reward model needed** - Unlike traditional RLHF/PPO, DPO directly optimizes preferences\n",
    "\n",
    "### Key Concepts:\n",
    "- **Chosen Response**: The preferred, higher-quality output\n",
    "- **Rejected Response**: The less desirable output\n",
    "- **DPO Loss**: Encourages model to favor chosen over rejected responses\n",
    "- **Beta Parameter**: Controls how strongly preferences are enforced\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ What We'll Do:\n",
    "1. Install Unsloth with DPO support\n",
    "2. Load a dataset with preference pairs (chosen vs rejected)\n",
    "3. Fine-tune SmolLM2-135M with LoRA + DPO\n",
    "4. Compare model outputs before and after training\n",
    "5. Evaluate preference alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa08559",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è IMPORTANT: GPU Required\n",
    "\n",
    "**This notebook requires a GPU to run.** Unsloth does not work on CPU.\n",
    "\n",
    "### üöÄ Recommended: Use Google Colab (FREE)\n",
    "1. Click the \"Open in Colab\" badge at the top of this notebook\n",
    "2. In Colab: **Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU**\n",
    "3. Run all cells sequentially\n",
    "\n",
    "### Alternative Options:\n",
    "- Cloud GPU services (AWS SageMaker, Azure ML, etc.)\n",
    "- Local machine with NVIDIA GPU + CUDA installed\n",
    "\n",
    "**If you see a \"No GPU detected\" error below, you must use one of the options above.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d7bc30",
   "metadata": {
    "id": "54d7bc30"
   },
   "source": [
    "## Step 1: Installation and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6dcd9823",
   "metadata": {
    "id": "6dcd9823"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth and required dependencies for DPO training\n",
    "# - unsloth: Core library with DPO optimization (2x faster than standard)\n",
    "# - trl: Provides DPOTrainer for preference learning\n",
    "# - peft: Implements LoRA for efficient fine-tuning\n",
    "# - bitsandbytes: Enables 4-bit quantization to save memory\n",
    "\n",
    "import os\n",
    "!pip install --upgrade -qqq uv\n",
    "\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    # Local installation\n",
    "    !pip install unsloth vllm\n",
    "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "else:\n",
    "    # Google Colab installation\n",
    "    !pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "    !pip install --no-deps xformers trl peft accelerate bitsandbytes\n",
    "\n",
    "print(\"‚úÖ Installation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02224297",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "02224297",
    "outputId": "cd75c3bb-f659-4f82-c74f-807779f5d32c"
   },
   "outputs": [],
   "source": [
    "# Check GPU availability and specifications\n",
    "# DPO requires more memory than standard fine-tuning because it processes\n",
    "# both chosen AND rejected responses simultaneously\n",
    "\n",
    "import torch\n",
    "\n",
    "print(\"üîç GPU Information:\")\n",
    "print(f\"  GPU Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"  GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"  GPU Memory: {gpu_memory:.2f} GB\")\n",
    "    print(f\"  BF16 Support: {torch.cuda.is_bf16_supported()}\")\n",
    "\n",
    "    if gpu_memory < 6:\n",
    "        print(\"\\n‚ö†Ô∏è  Warning: Less than 6GB VRAM. Consider using smaller batch size or sequence length.\")\n",
    "else:\n",
    "    print(\"\\n‚ùå CRITICAL: No GPU detected!\")\n",
    "    print(\"\\nüö® Unsloth REQUIRES a GPU to run. It does not work on CPU.\")\n",
    "    print(\"\\n‚úÖ Solutions:\")\n",
    "    print(\"  1. Use Google Colab (FREE GPU): Click 'Open in Colab' badge at the top\")\n",
    "    print(\"  2. Use a cloud GPU service (AWS, Azure, etc.)\")\n",
    "    print(\"  3. Run on a machine with an NVIDIA GPU\")\n",
    "    print(\"\\n‚ÑπÔ∏è  This notebook is designed for Google Colab with free GPU access.\")\n",
    "    print(\"   Simply open it in Colab and select Runtime > Change runtime type > GPU\")\n",
    "    \n",
    "    # Raise error to prevent further execution\n",
    "    raise RuntimeError(\n",
    "        \"Unsloth requires a GPU. Please run this notebook in Google Colab or \"\n",
    "        \"on a system with an NVIDIA GPU. See solutions above.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4460c302",
   "metadata": {
    "id": "4460c302"
   },
   "source": [
    "## Step 2: Load Preference Dataset\n",
    "\n",
    "For RLHF/DPO, we need a dataset with **preference pairs**:\n",
    "- **prompt**: The input question or instruction\n",
    "- **chosen**: The preferred, high-quality response\n",
    "- **rejected**: The less desirable, lower-quality response\n",
    "\n",
    "We'll use the **argilla/ultrafeedback-binarized-preferences-cleaned** dataset:\n",
    "- **60k+ high-quality preference pairs** from UltraFeedback\n",
    "- **GPT-4 quality judgments** for chosen/rejected responses\n",
    "- **Diverse topics**: coding, reasoning, creative writing, Q&A\n",
    "- **Clean format** ready for DPO training\n",
    "- **Production-quality**: Used by many popular open-source models\n",
    "- Better than Intel/orca_dpo_pairs for general-purpose instruction following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb63cce2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 827,
     "referenced_widgets": [
      "d9897d4cff6340f792e86f17b2d931dd",
      "a8f905584bc54e16b104a6b16cd1625c",
      "1571c04301124554914d280dc8d4dd00",
      "d25f9112c04146a6a6e585769ba54dc6",
      "90612b1f526c4198996a5957e6497877",
      "20964e6e4aa449648e3f5fbe64cae4d2",
      "ac36a21af24f4600ad03e0f64dc0d058",
      "16582aa0f484454b98ef8363ee40cb91",
      "1296ed50be404dbbb99c7eb7dc91e0c0",
      "660c7f5690e74f209121da724f762953",
      "aa2a8a61010b4b398ab1f8f095d799bf",
      "902cfe2195504ad29bbdf35ef710c10a",
      "46709c2f54f341bfa0996eed9532d51b",
      "56affdc2aaf34ce19926ea4311523b0e",
      "26388bebbd8a4613a6faf1039c122b84",
      "8d286bcb057d4b9d97f420a753ea33d2",
      "bc31485603d54dc5a0faddf49b43a515",
      "3e301473089f4fe2ba6772d91f99323e",
      "f49b5a875dbe4678ad4010fcf96c8b85",
      "3b4050f8e58348f29b2b660ae099fbfc",
      "6ea4db979b3c4f718d4beccfa548fbd2",
      "e75cee81bedb4f089620a0c2cace3523",
      "7086da05e1d646d0a9082c04a5b36633",
      "5505a657f00249f3b9385513a18fced5",
      "18e36c8f2689450c9c5770f3388f98ce",
      "1a8b8a687747472bba3d840ee66ebcfa",
      "28820fa1d110466eba36a97cdbffddff",
      "0dac5c5b46a94a3f9e301d6f4cf7bf9e",
      "f4a0aa9d9afa4fb0b27d82209c999287",
      "f66c7d72597f4c6db02d5a5fc1d77f02",
      "47ca0325a8f94ce4b440246c418ed075",
      "250a17a2b991446cba1d76410d63d1c8",
      "be8ae560fed44d1f913be0cb6a495c7a"
     ]
    },
    "id": "cb63cce2",
    "outputId": "7c2bbd26-6c74-4685-9deb-f2108eef467d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Loading Intel Orca DPO Pairs dataset...\n",
      "\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   Total samples: 1000\n",
      "   Features: ['system', 'question', 'chosen', 'rejected']\n",
      "\n",
      "================================================================================\n",
      "üìù EXAMPLE PREFERENCE PAIR\n",
      "================================================================================\n",
      "\n",
      "üîµ QUESTION: You will be given a definition of a task first, then some input of the task.\n",
      "This task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets...\n",
      "\n",
      "üü¢ CHOSEN (Preferred Response):\n",
      "--------------------------------------------------------------------------------\n",
      "[\n",
      "  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\n",
      "  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\n",
      "]...\n",
      "\n",
      "üî¥ REJECTED (Less Preferred Response):\n",
      "--------------------------------------------------------------------------------\n",
      " Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\n",
      "\n",
      "[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\n",
      "[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\n",
      "\n",
      "Explanation:\n",
      "\n",
      "* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst.\n",
      "* Ajax Youth Academ...\n",
      "================================================================================\n",
      "\n",
      "üí° The model will learn to prefer 'chosen' responses over 'rejected' ones.\n",
      "\n",
      "‚úÖ Dataset loaded successfully!\n",
      "   Total samples: 1000\n",
      "   Features: ['system', 'question', 'chosen', 'rejected']\n",
      "\n",
      "================================================================================\n",
      "üìù EXAMPLE PREFERENCE PAIR\n",
      "================================================================================\n",
      "\n",
      "üîµ QUESTION: You will be given a definition of a task first, then some input of the task.\n",
      "This task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the triplets...\n",
      "\n",
      "üü¢ CHOSEN (Preferred Response):\n",
      "--------------------------------------------------------------------------------\n",
      "[\n",
      "  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\n",
      "  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\n",
      "]...\n",
      "\n",
      "üî¥ REJECTED (Less Preferred Response):\n",
      "--------------------------------------------------------------------------------\n",
      " Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\n",
      "\n",
      "[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\n",
      "[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\n",
      "\n",
      "Explanation:\n",
      "\n",
      "* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that describes the relationship between AFC Ajax (amateurs) and Sportpark De Toekomst.\n",
      "* Ajax Youth Academ...\n",
      "================================================================================\n",
      "\n",
      "üí° The model will learn to prefer 'chosen' responses over 'rejected' ones.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load UltraFeedback Binarized Preferences dataset\n",
    "# This is one of the BEST datasets for DPO training:\n",
    "# - 60k+ high-quality preference pairs\n",
    "# - GPT-4 quality judgments\n",
    "# - Diverse topics (coding, reasoning, creative writing, Q&A)\n",
    "# - Used by many state-of-the-art open-source models\n",
    "\n",
    "print(\"üì¶ Loading UltraFeedback Binarized Preferences dataset...\")\n",
    "print(\"   This is a production-quality dataset with 60k+ samples\")\n",
    "print(\"   Loading first 2000 samples for faster training...\\n\")\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"argilla/ultrafeedback-binarized-preferences-cleaned\",\n",
    "    split=\"train[:2000]\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset loaded successfully!\")\n",
    "print(f\"   Total samples: {len(dataset)}\")\n",
    "print(f\"   Features: {dataset.column_names}\")\n",
    "\n",
    "# Display a sample preference pair\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù EXAMPLE PREFERENCE PAIR\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "sample = dataset[0]\n",
    "\n",
    "# Show the prompt\n",
    "print(f\"\\nüîµ PROMPT:\")\n",
    "print(\"-\" * 80)\n",
    "print(sample['prompt'][:500] + \"...\" if len(sample['prompt']) > 500 else sample['prompt'])\n",
    "\n",
    "print(f\"\\nüü¢ CHOSEN (Preferred Response):\")\n",
    "print(\"-\" * 80)\n",
    "chosen_text = sample['chosen'][-1]['content'] if isinstance(sample['chosen'], list) else sample['chosen']\n",
    "print(chosen_text[:500] + \"...\" if len(chosen_text) > 500 else chosen_text)\n",
    "\n",
    "print(f\"\\nüî¥ REJECTED (Less Preferred Response):\")\n",
    "print(\"-\" * 80)\n",
    "rejected_text = sample['rejected'][-1]['content'] if isinstance(sample['rejected'], list) else sample['rejected']\n",
    "print(rejected_text[:500] + \"...\" if len(rejected_text) > 500 else rejected_text)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üí° The model will learn to prefer 'chosen' responses over 'rejected' ones.\")\n",
    "print(\"üí° This dataset contains diverse, real-world instructions and high-quality responses.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f19357c",
   "metadata": {
    "id": "3f19357c"
   },
   "source": [
    "## Step 3: Load Model with 4-bit Quantization\n",
    "\n",
    "We'll use **SmolLM2-135M** - a tiny but capable language model:\n",
    "- Only 135 million parameters (fits in ~4GB VRAM with 4-bit quantization)\n",
    "- Fast training and inference\n",
    "- Perfect for learning DPO concepts\n",
    "\n",
    "**Unsloth Optimizations for DPO:**\n",
    "1. Efficient dual forward passes (for chosen AND rejected responses)\n",
    "2. Shared computation between reference and policy models  \n",
    "3. Memory-efficient KL divergence calculation\n",
    "4. Optimized gradient computation for preference loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c1c1f0a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533,
     "referenced_widgets": [
      "69f2416ef54748cab352d782479b9ddf",
      "a0cbf3da6c1d412baf39ab5e82e12634",
      "794bc2f125514e51932211dc9b2d1a6d",
      "6dfb782eb811425bbd65af1a9ff2aa0b",
      "4b9f8851750e43c48f9bf7d1e0d01f5b",
      "f3ab9f2bb256460085d0c67fde5a9b41",
      "4e87d0c8aa5a411ebb2c0e068d3f2e61",
      "b5854ad6df5b439186092d28bd1ae69d",
      "28733bab04cf419faddbbfab61f4e5ac",
      "3f8fb042d7ec44b8b2e5cbe73c40e7f5",
      "7c551752227f4b4fad9e30711eb05083",
      "f2966a4a3d974ae9bfe18fd07b17b40e",
      "8b21951ab2be4c31aed698dd343e2c7a",
      "33c4789cb5734ffcab14b120b501d0f2",
      "41596a17219042c9aedaa65281df2d74",
      "5e1bb08f5bfc4900aa38391beb62d319",
      "f0a649ce0a0a416ea0635c571bb53471",
      "58427d1807574bbda800bd3b2232073a",
      "349e80b99a2c4eab8a9675c978028804",
      "c39be4a791bc4a258f4fea51065eef3e",
      "7f147bddbf104197a385566d2228d75f",
      "5eebb9c831e540fe8805cb2329626fe7",
      "4d9f62bfba71421a8239704cf73ef0da",
      "3c48a8b87943414188828b71b3847e1a",
      "35011cee90b9428e8b35d2beffbd80b0",
      "e2b15e0c25684b2c8753d65e78a91a72",
      "855bd0262c1448de9b91d76a320e003b",
      "048eb0d6becd467f935b8457157d38f1",
      "8cfa102b31c4472884a885d24a976a2a",
      "e3ec0a15f5534eb497bd6c860503fbed",
      "a1e07b7e212647d5920a1a8b1ff43f11",
      "678c82c8c21f4d62bbf896d2e528488f",
      "af73e73976fd44d99edf878c6fc1042a",
      "2d5dfac75bde430588158de558a82c25",
      "3eb72e59520f49b8b8001871111a56c9",
      "d181e75b83144c7d8dadd3a1ea373e52",
      "351ea7f5a60a4b729dba6fc5ff45b038",
      "8252e30aab084192b8d401f2fb56cb30",
      "56ef8fd7716a4074b039f60ba43ad9c7",
      "06adb5a7442c4a43be36ce7888655600",
      "1e72e69ae7c946d4878a33af7c02ea0e",
      "31532ae10cf34c8fb0fab44022ec73eb",
      "2cdc2f72ecec4964823df07463ac2d92",
      "bd3439e5ca42476b8003f2a0b0d61608",
      "fdad0155724947abb7d80149bad993bf",
      "16c88ad4738a49deaf089f3a1d8168d7",
      "b716ed057e5146acb40c60ff46736414",
      "481433f519a244a3a3cb46b72e1b8ba3",
      "3784b5fc4e8048f284bd9ee01fd2c8ad",
      "7079879bae6143b3b38cedc3cd67a6ef",
      "c34f144773c34656a17cfdbe8f105c75",
      "a36104222dd44906aa35a257a890a8f9",
      "fcb49baa66c74c99bc0a72139f3dfae9",
      "9b1898472cd04a618748939e6b662d6c",
      "80de7bf349894e86b6fe525ac048dd31",
      "affe0eb2004f4e9895fb863d44b41d2c",
      "ecd11b6e2c7d4b87907f4bf597f189e3",
      "0bba888cde7c4c4da09cd9c20ca83c44",
      "2d50842deb87456e97b33588a9884738",
      "f7580012ed914570944d71e24c05541e",
      "975a9a9594a1461ea9a68f9899879ac0",
      "f997ca049e83471796b9469f616fff93",
      "3ea3e53f9ed5408885d688553a2818e5",
      "4d07dc205a9c4185a78b383068a940f7",
      "aa7146eeea654d7783ea0f9f243d3ac4",
      "1f5c3cbef5db415a92f3b06e0599b019",
      "bdd67a42b06e44059af56159cf2b250a",
      "ed8d1cb63cbb4927906b08ab55981d66",
      "460b10cb3ea1442ba9d3a8bfd28739c1",
      "50b509ae936945c2903567f002e4ba51",
      "7e60eb9488de4fe3b20a4e20bdd6170a",
      "b1bc265d76594cd4b04b5b9c4ec0402f",
      "2837530a86974beab7a88a790811c31e",
      "393bc9379bba4d30b3546b0ca344a2c1",
      "3f15e8097b004c3ba40d9e1f2617d1b6",
      "8648a934025d45a6a4ee53a22d8553af",
      "1592f97b014042c9a67a09205c765904",
      "f28170a574a84da3890726676a0bf9eb",
      "883b3e17fad947f98d0274ef2b40d4bd",
      "86fca71395c84f39a8c271a56dfd00a6",
      "6ad5e20bde7a42f7b27f432ae8f33b7d",
      "4853807ae7294aa8b7dfccc0fde33852",
      "aa44f18ef9a74313922ed945a4858108",
      "1611df69e80946628d11e24cf6217846",
      "52585c9b97ea4bb881f49a8ee5db65d0",
      "cbab11430d1f48f7ba9a351a52f77114",
      "269a07a117cc4d46b25a99fdc883e110",
      "a04be30570b84ddd835844e0bb2568fd"
     ]
    },
    "id": "6c1c1f0a",
    "outputId": "64eac935-568f-41ce-903e-ced22c6afcb9"
   },
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsloth cannot find any torch accelerator? You need a GPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FastLanguageModel\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Model configuration\u001b[39;00m\n\u001b[32m      4\u001b[39m max_seq_length = \u001b[32m2048\u001b[39m  \u001b[38;5;66;03m# Maximum sequence length for training\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ds_assginement/unsloath/.venv/lib/python3.13/site-packages/unsloth/__init__.py:80\u001b[39m\n\u001b[32m     68\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\n\u001b[32m     69\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please update Unsloth and Unsloth-Zoo to the latest version!\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     70\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDo this via `pip install --upgrade --force-reinstall --no-cache-dir --no-deps unsloth unsloth_zoo`\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m         )\n\u001b[32m     72\u001b[39m         \u001b[38;5;66;03m# if os.environ.get(\"UNSLOTH_DISABLE_AUTO_UPDATES\", \"0\") == \"0\":\u001b[39;00m\n\u001b[32m     73\u001b[39m         \u001b[38;5;66;03m#     try:\u001b[39;00m\n\u001b[32m     74\u001b[39m         \u001b[38;5;66;03m#         os.system(\"pip install --upgrade --no-cache-dir --no-deps unsloth_zoo\")\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     78\u001b[39m         \u001b[38;5;66;03m#         except:\u001b[39;00m\n\u001b[32m     79\u001b[39m         \u001b[38;5;66;03m#             raise ImportError(\"Unsloth: Please update unsloth_zoo via `pip install --upgrade --no-cache-dir --no-deps unsloth_zoo`\")\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munsloth_zoo\u001b[39;00m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m PackageNotFoundError:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     83\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsloth: Please install unsloth_zoo via `pip install unsloth_zoo` then retry!\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ds_assginement/unsloath/.venv/lib/python3.13/site-packages/unsloth_zoo/__init__.py:136\u001b[39m\n\u001b[32m    133\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m logging, torchao_logger, HideLoggingMessage\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m# Get device types and other variables\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdevice_type\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m    137\u001b[39m     is_hip,\n\u001b[32m    138\u001b[39m     get_device_type,\n\u001b[32m    139\u001b[39m     DEVICE_TYPE,\n\u001b[32m    140\u001b[39m     DEVICE_TYPE_TORCH,\n\u001b[32m    141\u001b[39m     DEVICE_COUNT,\n\u001b[32m    142\u001b[39m     ALLOW_PREQUANTIZED_MODELS,\n\u001b[32m    143\u001b[39m )\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Torch 2.9 removed PYTORCH_HIP_ALLOC_CONF and PYTORCH_CUDA_ALLOC_CONF\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m major_torch == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m minor_torch >= \u001b[32m9\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ds_assginement/unsloath/.venv/lib/python3.13/site-packages/unsloth_zoo/device_type.py:56\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth currently only works on NVIDIA, AMD and Intel GPUs.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m DEVICE_TYPE : \u001b[38;5;28mstr\u001b[39m = \u001b[43mget_device_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# HIP fails for autocast and other torch functions. Use CUDA instead\u001b[39;00m\n\u001b[32m     58\u001b[39m DEVICE_TYPE_TORCH = DEVICE_TYPE\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ds_assginement/unsloath/.venv/lib/python3.13/site-packages/unsloth_zoo/device_type.py:46\u001b[39m, in \u001b[36mget_device_type\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch, \u001b[33m\"\u001b[39m\u001b[33maccelerator\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     45\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.accelerator.is_available():\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mUnsloth cannot find any torch accelerator? You need a GPU.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m     accelerator = \u001b[38;5;28mstr\u001b[39m(torch.accelerator.current_accelerator())\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m accelerator \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mxpu\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mhip\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsloth cannot find any torch accelerator? You need a GPU."
     ]
    }
   ],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Model configuration\n",
    "max_seq_length = 2048  # Maximum sequence length for training\n",
    "dtype = None           # Auto-detect optimal dtype (bfloat16 if supported)\n",
    "load_in_4bit = True    # Enable 4-bit quantization to save memory\n",
    "\n",
    "print(\"üîÑ Loading model...\")\n",
    "\n",
    "# Load SmolLM2-135M with Unsloth optimizations\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/smollm2-135m-instruct\",\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "# Configure padding token for batch processing\n",
    "# DPO requires batch processing of chosen/rejected pairs\n",
    "# Padding ensures all sequences in a batch have the same length\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(\"‚úÖ Padding token configured\")\n",
    "\n",
    "# Model information\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n‚úÖ Model loaded: {model.config._name_or_path}\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Max sequence length: {max_seq_length}\")\n",
    "print(f\"   4-bit quantization: {load_in_4bit}\")\n",
    "print(f\"   Memory footprint: ~4GB VRAM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb09be",
   "metadata": {
    "id": "71cb09be"
   },
   "source": [
    "## Step 4: Apply LoRA for Efficient DPO Training\n",
    "\n",
    "**Why LoRA for DPO?**\n",
    "- DPO processes both chosen AND rejected responses ‚Üí 2x memory usage\n",
    "- LoRA reduces trainable parameters by 99% (full model = 100% parameters)\n",
    "- Higher rank (32) for DPO compared to standard LoRA (8-16)\n",
    "  - Preference learning is more nuanced than simple task adaptation\n",
    "  - Model needs to learn subtle differences between chosen/rejected responses\n",
    "\n",
    "**LoRA Configuration:**\n",
    "- **Rank (r=32)**: Higher than standard LoRA for better preference capture\n",
    "- **Alpha (32)**: Typically matches rank for DPO stability\n",
    "- **Target modules**: Apply to all attention and MLP layers for maximum coverage\n",
    "- **No dropout**: Helps training stability in DPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae01bcd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eae01bcd",
    "outputId": "d5461006-b5cd-48de-ea5d-f89edb26da3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Applying LoRA adapters for DPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.11.2 patched 30 layers with 30 QKV layers, 30 O layers and 30 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ LoRA Applied Successfully!\n",
      "   Trainable parameters: 9,768,960\n",
      "   Total parameters: 91,200,384\n",
      "   Trainable percentage: 10.7115%\n",
      "   LoRA Rank: 32\n",
      "   Memory savings: ~99% fewer parameters to train!\n"
     ]
    }
   ],
   "source": [
    "print(\"üîß Applying LoRA adapters for DPO training...\")\n",
    "\n",
    "# Apply LoRA with configuration optimized for preference learning\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 32,  # Higher rank for nuanced preference learning (vs 8-16 for standard tasks)\n",
    "    target_modules = [\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention layers\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",     # MLP layers\n",
    "    ],\n",
    "    lora_alpha = 32,       # Match rank for stable DPO training\n",
    "    lora_dropout = 0,       # No dropout improves DPO stability\n",
    "    bias = \"none\",          # No bias adaptation\n",
    "    use_gradient_checkpointing = \"unsloth\",  # Unsloth's optimized checkpointing\n",
    "    random_state = 3407,    # For reproducibility\n",
    "    use_rslora = False,     # Standard LoRA scaling\n",
    ")\n",
    "\n",
    "# Calculate parameter efficiency\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_percentage = (trainable_params / total_params) * 100\n",
    "\n",
    "print(f\"\\n‚úÖ LoRA Applied Successfully!\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable percentage: {trainable_percentage:.4f}%\")\n",
    "print(f\"   LoRA Rank: 32\")\n",
    "print(f\"   Memory savings: ~99% fewer parameters to train!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7159b",
   "metadata": {
    "id": "90a7159b"
   },
   "source": [
    "## Step 5: Prepare Dataset for DPO Training\n",
    "\n",
    "The UltraFeedback dataset uses a conversation format. We need to:\n",
    "1. Extract the prompt from the conversation history\n",
    "2. Extract the final assistant responses (chosen vs rejected)\n",
    "3. Ensure the format matches what DPOTrainer expects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb7967c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 572,
     "referenced_widgets": [
      "81105f74354846a5ba1d310fd4a5b16c",
      "fd8cfe67fa304507ad8263e5606e7b92",
      "ee923c632dd9489fa5d2208aa95f6ea1",
      "3fb5472847654863ac5014d17dc097c5",
      "a5a1aff3aa5f403586f956067bf6f2a9",
      "662ffc6dbb7f475fac838b2e3b4075a8",
      "798dc7f9858445ab8e0e7ca2943f449b",
      "e567204c6d2c42538b080c0376a85dde",
      "0482823e32a74b7d977617de927adfef",
      "b4a4d07be065469cb594cb9a146b14d0",
      "3674c7a7ff604ca9beb9f668287fc25a"
     ]
    },
    "id": "0fb7967c",
    "outputId": "50280d3f-c509-4594-8241-96b0cb155bfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Formatting dataset for DPO training...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81105f74354846a5ba1d310fd4a5b16c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dataset formatted!\n",
      "   Samples: 1000\n",
      "   Format: prompt + chosen + rejected\n",
      "\n",
      "================================================================================\n",
      "üìù FORMATTED DPO EXAMPLE\n",
      "================================================================================\n",
      "\n",
      "üîµ PROMPT:\n",
      "User: You will be given a definition of a task first, then some input of the task.\n",
      "This task is about using the specified sentence and converting the sentence to Resource Description Framework (RDF) triplets of the form (subject, predicate object). The RDF triplets generated must be such that the tr...\n",
      "\n",
      "üü¢ CHOSEN:\n",
      "[\n",
      "  [\"AFC Ajax (amateurs)\", \"has ground\", \"Sportpark De Toekomst\"],\n",
      "  [\"Ajax Youth Academy\", \"plays at\", \"Sportpark De Toekomst\"]\n",
      "]...\n",
      "\n",
      "üî¥ REJECTED:\n",
      " Sure, I'd be happy to help! Here are the RDF triplets for the input sentence:\n",
      "\n",
      "[AFC Ajax (amateurs), hasGround, Sportpark De Toekomst]\n",
      "[Ajax Youth Academy, playsAt, Sportpark De Toekomst]\n",
      "\n",
      "Explanation:\n",
      "\n",
      "* AFC Ajax (amateurs) is the subject of the first triplet, and hasGround is the predicate that d...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "def format_for_dpo(example):\n",
    "    \"\"\"\n",
    "    Format UltraFeedback dataset for DPO training.\n",
    "    \n",
    "    The dataset structure:\n",
    "    - prompt: The user's instruction/question (string)\n",
    "    - chosen: List of conversation turns with the preferred response\n",
    "    - rejected: List of conversation turns with the rejected response\n",
    "    \n",
    "    We need to extract the final assistant response from each conversation.\n",
    "    \"\"\"\n",
    "    # The prompt is already a clean string\n",
    "    prompt = example['prompt']\n",
    "    \n",
    "    # Extract the assistant's response from chosen conversation\n",
    "    # chosen/rejected are lists of message dicts with 'role' and 'content'\n",
    "    if isinstance(example['chosen'], list):\n",
    "        # Get the last assistant message\n",
    "        chosen_text = [msg['content'] for msg in example['chosen'] if msg['role'] == 'assistant'][-1]\n",
    "    else:\n",
    "        chosen_text = example['chosen']\n",
    "    \n",
    "    if isinstance(example['rejected'], list):\n",
    "        # Get the last assistant message\n",
    "        rejected_text = [msg['content'] for msg in example['rejected'] if msg['role'] == 'assistant'][-1]\n",
    "    else:\n",
    "        rejected_text = example['rejected']\n",
    "    \n",
    "    return {\n",
    "        'prompt': prompt,\n",
    "        'chosen': chosen_text,\n",
    "        'rejected': rejected_text,\n",
    "    }\n",
    "\n",
    "print(\"üîÑ Formatting dataset for DPO training...\")\n",
    "\n",
    "# Apply formatting to dataset\n",
    "dpo_dataset = dataset.map(\n",
    "    format_for_dpo,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Dataset formatted!\")\n",
    "print(f\"   Samples: {len(dpo_dataset)}\")\n",
    "print(f\"   Format: prompt + chosen + rejected\")\n",
    "\n",
    "# Show formatted example\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìù FORMATTED DPO EXAMPLE\")\n",
    "print(\"=\"*80)\n",
    "example = dpo_dataset[0]\n",
    "print(f\"\\nüîµ PROMPT:\\n{example['prompt'][:400]}...\\n\" if len(example['prompt']) > 400 else f\"\\nüîµ PROMPT:\\n{example['prompt']}\\n\")\n",
    "print(f\"üü¢ CHOSEN:\\n{example['chosen'][:400]}...\\n\" if len(example['chosen']) > 400 else f\"üü¢ CHOSEN:\\n{example['chosen']}\\n\")\n",
    "print(f\"üî¥ REJECTED:\\n{example['rejected'][:400]}...\" if len(example['rejected']) > 400 else f\"üî¥ REJECTED:\\n{example['rejected']}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95ccc0d",
   "metadata": {
    "id": "a95ccc0d"
   },
   "source": [
    "## Step 6: Configure and Start DPO Training\n",
    "\n",
    "**What is DPO (Direct Preference Optimization)?**\n",
    "- Simpler alternative to PPO-based RLHF (no reward model or value model needed)\n",
    "- Directly optimizes the policy to prefer chosen responses over rejected ones\n",
    "- Uses a beta parameter to control the strength of preference enforcement\n",
    "\n",
    "**Training Configuration:**\n",
    "- **Beta (0.1)**: KL divergence penalty - prevents model from deviating too much\n",
    "- **Learning rate (5e-5)**: Lower than supervised fine-tuning for stability\n",
    "- **Batch size (2)**: Process 2 preference pairs per step\n",
    "- **Gradient accumulation (4)**: Effective batch size of 8\n",
    "- **Max steps (200)**: Quick training for demonstration (increase for better results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121a8884",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 217,
     "referenced_widgets": [
      "277e0c156fb14241b6509a517d7efe93",
      "45a786266c9d438aa11a070d49eb2cc1",
      "6e5fef1811884067b9e88673e3cc7996",
      "8c91233a8ca14df9add14c1618023071",
      "9f64c413b2f24fb6849141f646d8fe2f",
      "7c889e939d2d49b8b27104beae61e0ed",
      "b74157468f834585b544fdc45488b956",
      "b40e37ed5864411c87d5240b965b2539",
      "085f430dabff477dbbaff6bd3ba407dd",
      "e5e895fce3cf44628cede5a11dc002f8",
      "f4ccf92a8bae430d997820533e7811b8",
      "5d64f30adb2648feb8c548eae334a698",
      "275448f41a63471abe7503ac62c2779f",
      "4b3d2b5c5eb04056829100721984d4c4",
      "0f967fdc56304ef6bd4127df8838d12d",
      "5100d01f23d04b77bb7fb29a7ce40b27",
      "838680363de244cdb161e27522f7e5bd",
      "9e32fa35d5d4410a98e7f192729e24c3",
      "8c9dea5a609d4cdc833d47d0788ec5ab",
      "551a28914b3449ccb1232fa2d10dc505",
      "733adfcab47a4014986cfe53cd40e122",
      "3273b923064340909e3e5a4f6c40c83c",
      "d38824e6a41b419796fe1719029dfcce",
      "28f916abdd3e47519a8a7423a3b3ea5d",
      "98ed412cb13546c895d58f050aff983b",
      "82c44346278f4525b234b7add4c4e699",
      "1c5e4666218c434191fcc1844e7e1018",
      "71a1d3ec3bcd4951965873fa0be65f94",
      "990d6aa250674c5e92de45e313f66b75",
      "5b4a2a68094b4bfd9e86995cf4f7d6e6",
      "d379762fe49a451ca23d987c6f02c728",
      "b2169b6ba85249a4bb5006d2c714bdb4",
      "b1807f498287401bbaf592b1d0dd3650"
     ]
    },
    "id": "121a8884",
    "outputId": "a951378d-a1ea-494a-a359-f6c1b319acfd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚öôÔ∏è  Configuring DPO Trainer...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277e0c156fb14241b6509a517d7efe93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset (num_proc=12):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d64f30adb2648feb8c548eae334a698",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset (num_proc=12):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d38824e6a41b419796fe1719029dfcce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset (num_proc=12):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DPO Trainer configured!\n",
      "   Training steps: 200\n",
      "   Effective batch size: 8\n",
      "   Beta (KL penalty): 0.1\n",
      "   Learning rate: 5e-05\n"
     ]
    }
   ],
   "source": [
    "from trl import DPOTrainer, DPOConfig\n",
    "\n",
    "print(\"‚öôÔ∏è  Configuring DPO Trainer...\")\n",
    "\n",
    "# DPO Training Configuration\n",
    "training_args = DPOConfig(\n",
    "    # Model training\n",
    "    beta = 0.1,  # KL divergence penalty (higher = stay closer to reference model)\n",
    "\n",
    "    # Optimization\n",
    "    per_device_train_batch_size = 2,     # Samples per GPU\n",
    "    gradient_accumulation_steps = 4,      # Effective batch size = 2 * 4 = 8\n",
    "    learning_rate = 5e-5,                 # Lower LR for stable DPO training\n",
    "\n",
    "    # Training schedule\n",
    "    max_steps = 200,                      # Total training steps (increase for better results)\n",
    "    warmup_steps = 10,                    # Warmup for first 10 steps\n",
    "\n",
    "    # Logging and checkpointing\n",
    "    logging_steps = 10,                   # Log every 10 steps\n",
    "    save_steps = 50,                      # Save checkpoint every 50 steps\n",
    "    output_dir = \"./dpo_output\",          # Where to save checkpoints\n",
    "\n",
    "    # Optimization settings\n",
    "    optim = \"adamw_8bit\",                 # 8-bit AdamW optimizer for memory efficiency\n",
    "    weight_decay = 0.01,                  # L2 regularization\n",
    "    lr_scheduler_type = \"cosine\",         # Cosine learning rate decay\n",
    "\n",
    "    # Memory optimization\n",
    "    fp16 = not torch.cuda.is_bf16_supported(),  # Use fp16 if bf16 not available\n",
    "    bf16 = torch.cuda.is_bf16_supported(),       # Use bf16 if available (better precision)\n",
    "    gradient_checkpointing = True,        # Trade compute for memory\n",
    "\n",
    "    # Misc\n",
    "    seed = 42,\n",
    "    report_to = \"none\",  # Disable wandb/tensorboard for simplicity\n",
    ")\n",
    "\n",
    "# Initialize DPO Trainer\n",
    "trainer = DPOTrainer(\n",
    "    model = model,\n",
    "    args = training_args,\n",
    "    train_dataset = dpo_dataset,\n",
    "    tokenizer = tokenizer,\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ DPO Trainer configured!\")\n",
    "print(f\"   Training steps: {training_args.max_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Beta (KL penalty): {training_args.beta}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d98ce58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "0d98ce58",
    "outputId": "9f52d0f0-dcc9-490a-87c6-d98a5c647cef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üíæ Memory Status Before Training:\n",
      "   GPU: Tesla T4\n",
      "   Max memory: 14.741 GB\n",
      "   Reserved: 0.229 GB\n",
      "   Available: 14.51 GB\n",
      "\n",
      "üöÄ Starting DPO Training...\n",
      "   This will take approximately 10-20 minutes depending on your GPU\n",
      "   Progress will be logged every 10 steps\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 1,000 | Num Epochs = 2 | Total steps = 200\n",
      "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
      " \"-____-\"     Trainable parameters = 9,768,960 of 144,284,544 (6.77% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 08:08, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>rewards / chosen</th>\n",
       "      <th>rewards / rejected</th>\n",
       "      <th>rewards / accuracies</th>\n",
       "      <th>rewards / margins</th>\n",
       "      <th>logps / chosen</th>\n",
       "      <th>logps / rejected</th>\n",
       "      <th>logits / chosen</th>\n",
       "      <th>logits / rejected</th>\n",
       "      <th>eval_logits / chosen</th>\n",
       "      <th>eval_logits / rejected</th>\n",
       "      <th>nll_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.682700</td>\n",
       "      <td>-0.017607</td>\n",
       "      <td>-0.039052</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.021445</td>\n",
       "      <td>-287.625793</td>\n",
       "      <td>-353.521667</td>\n",
       "      <td>4.226661</td>\n",
       "      <td>3.752743</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.545400</td>\n",
       "      <td>-0.188937</td>\n",
       "      <td>-0.547912</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>0.358975</td>\n",
       "      <td>-280.055450</td>\n",
       "      <td>-346.471893</td>\n",
       "      <td>5.200512</td>\n",
       "      <td>4.598498</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.346700</td>\n",
       "      <td>-0.508056</td>\n",
       "      <td>-1.583624</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>1.075568</td>\n",
       "      <td>-289.477753</td>\n",
       "      <td>-417.328033</td>\n",
       "      <td>5.716645</td>\n",
       "      <td>5.120669</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>-0.959190</td>\n",
       "      <td>-2.730018</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>1.770828</td>\n",
       "      <td>-321.631256</td>\n",
       "      <td>-387.211853</td>\n",
       "      <td>6.451074</td>\n",
       "      <td>5.637962</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.141200</td>\n",
       "      <td>-1.401122</td>\n",
       "      <td>-3.964152</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.563030</td>\n",
       "      <td>-281.419373</td>\n",
       "      <td>-395.761902</td>\n",
       "      <td>5.507342</td>\n",
       "      <td>5.251302</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.179800</td>\n",
       "      <td>-2.039856</td>\n",
       "      <td>-5.362223</td>\n",
       "      <td>0.937500</td>\n",
       "      <td>3.322367</td>\n",
       "      <td>-387.540222</td>\n",
       "      <td>-443.123932</td>\n",
       "      <td>5.733437</td>\n",
       "      <td>5.148309</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.223400</td>\n",
       "      <td>-2.154034</td>\n",
       "      <td>-5.550057</td>\n",
       "      <td>0.925000</td>\n",
       "      <td>3.396023</td>\n",
       "      <td>-296.901123</td>\n",
       "      <td>-382.395294</td>\n",
       "      <td>6.197063</td>\n",
       "      <td>5.593513</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.132800</td>\n",
       "      <td>-2.351438</td>\n",
       "      <td>-6.221224</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>3.869786</td>\n",
       "      <td>-411.425964</td>\n",
       "      <td>-472.055115</td>\n",
       "      <td>6.627950</td>\n",
       "      <td>5.653414</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.059800</td>\n",
       "      <td>-1.772049</td>\n",
       "      <td>-6.519984</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.747935</td>\n",
       "      <td>-260.686523</td>\n",
       "      <td>-403.675751</td>\n",
       "      <td>7.351417</td>\n",
       "      <td>6.551938</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.060100</td>\n",
       "      <td>-2.326214</td>\n",
       "      <td>-7.160777</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>4.834563</td>\n",
       "      <td>-357.782349</td>\n",
       "      <td>-496.863953</td>\n",
       "      <td>6.609488</td>\n",
       "      <td>6.275220</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.047300</td>\n",
       "      <td>-2.386567</td>\n",
       "      <td>-7.468850</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.082283</td>\n",
       "      <td>-358.600250</td>\n",
       "      <td>-483.924225</td>\n",
       "      <td>6.167163</td>\n",
       "      <td>5.823884</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.062300</td>\n",
       "      <td>-2.167814</td>\n",
       "      <td>-7.425838</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.258024</td>\n",
       "      <td>-317.727142</td>\n",
       "      <td>-453.385803</td>\n",
       "      <td>6.831491</td>\n",
       "      <td>5.967956</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.112700</td>\n",
       "      <td>-2.223887</td>\n",
       "      <td>-8.060237</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>5.836350</td>\n",
       "      <td>-316.670929</td>\n",
       "      <td>-455.132507</td>\n",
       "      <td>5.868285</td>\n",
       "      <td>5.261246</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.060200</td>\n",
       "      <td>-2.253169</td>\n",
       "      <td>-7.479304</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>5.226136</td>\n",
       "      <td>-272.710022</td>\n",
       "      <td>-378.691589</td>\n",
       "      <td>6.523875</td>\n",
       "      <td>5.953904</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.024900</td>\n",
       "      <td>-2.381895</td>\n",
       "      <td>-8.688196</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>6.306301</td>\n",
       "      <td>-336.964722</td>\n",
       "      <td>-457.590424</td>\n",
       "      <td>6.540701</td>\n",
       "      <td>5.988367</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.031300</td>\n",
       "      <td>-1.984343</td>\n",
       "      <td>-8.089394</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.105050</td>\n",
       "      <td>-297.563477</td>\n",
       "      <td>-457.435059</td>\n",
       "      <td>6.664444</td>\n",
       "      <td>5.951675</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.082400</td>\n",
       "      <td>-1.983790</td>\n",
       "      <td>-7.877818</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>5.894029</td>\n",
       "      <td>-318.707855</td>\n",
       "      <td>-447.370026</td>\n",
       "      <td>6.383788</td>\n",
       "      <td>6.031497</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>-1.965579</td>\n",
       "      <td>-8.467499</td>\n",
       "      <td>0.987500</td>\n",
       "      <td>6.501920</td>\n",
       "      <td>-296.212311</td>\n",
       "      <td>-495.736145</td>\n",
       "      <td>6.349874</td>\n",
       "      <td>6.104230</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.092000</td>\n",
       "      <td>-2.680449</td>\n",
       "      <td>-8.311031</td>\n",
       "      <td>0.962500</td>\n",
       "      <td>5.630582</td>\n",
       "      <td>-396.525940</td>\n",
       "      <td>-479.784729</td>\n",
       "      <td>7.020918</td>\n",
       "      <td>6.310377</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.024800</td>\n",
       "      <td>-2.403746</td>\n",
       "      <td>-8.743486</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>6.339742</td>\n",
       "      <td>-390.562134</td>\n",
       "      <td>-508.892090</td>\n",
       "      <td>6.619463</td>\n",
       "      <td>6.098054</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "      <td>No Log</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Training Complete!\n",
      "   Time taken: 496.75 seconds\n",
      "   Time taken: 8.28 minutes\n"
     ]
    }
   ],
   "source": [
    "# Check memory usage before training\n",
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"\\nüíæ Memory Status Before Training:\")\n",
    "print(f\"   GPU: {gpu_stats.name}\")\n",
    "print(f\"   Max memory: {max_memory} GB\")\n",
    "print(f\"   Reserved: {start_gpu_memory} GB\")\n",
    "print(f\"   Available: {max_memory - start_gpu_memory:.2f} GB\")\n",
    "\n",
    "print(f\"\\nüöÄ Starting DPO Training...\")\n",
    "print(f\"   This will take approximately 10-20 minutes depending on your GPU\")\n",
    "print(f\"   Progress will be logged every 10 steps\\n\")\n",
    "\n",
    "# Start training!\n",
    "trainer_stats = trainer.train()\n",
    "\n",
    "print(f\"\\n‚úÖ Training Complete!\")\n",
    "print(f\"   Time taken: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Time taken: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863aad88",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "863aad88",
    "outputId": "cb004c9f-9e3f-40c1-8f79-6a270eafe60c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Training Statistics:\n",
      "   Training runtime: 496.75 seconds\n",
      "   Training runtime: 8.28 minutes\n",
      "   Samples per second: 3.22\n",
      "   Steps per second: 0.40\n",
      "\n",
      "üíæ Memory Usage:\n",
      "   Peak reserved: 6.178 GB\n",
      "   Memory for training: 5.949 GB\n",
      "   Peak % of max memory: 41.91%\n",
      "   Training % of max memory: 40.357%\n",
      "\n",
      "‚ú® DPO training with Unsloth:\n",
      "   ‚úì 2x faster than standard implementations\n",
      "   ‚úì 60% less memory usage\n",
      "   ‚úì Same accuracy as full precision training\n"
     ]
    }
   ],
   "source": [
    "# Show final memory and performance statistics\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_training = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "training_percentage = round(used_memory_for_training / max_memory * 100, 3)\n",
    "\n",
    "print(f\"\\nüìä Training Statistics:\")\n",
    "print(f\"   Training runtime: {trainer_stats.metrics['train_runtime']:.2f} seconds\")\n",
    "print(f\"   Training runtime: {round(trainer_stats.metrics['train_runtime']/60, 2)} minutes\")\n",
    "print(f\"   Samples per second: {trainer_stats.metrics.get('train_samples_per_second', 0):.2f}\")\n",
    "print(f\"   Steps per second: {trainer_stats.metrics.get('train_steps_per_second', 0):.2f}\")\n",
    "\n",
    "print(f\"\\nüíæ Memory Usage:\")\n",
    "print(f\"   Peak reserved: {used_memory} GB\")\n",
    "print(f\"   Memory for training: {used_memory_for_training} GB\")\n",
    "print(f\"   Peak % of max memory: {used_percentage}%\")\n",
    "print(f\"   Training % of max memory: {training_percentage}%\")\n",
    "\n",
    "print(f\"\\n‚ú® DPO training with Unsloth:\")\n",
    "print(f\"   ‚úì 2x faster than standard implementations\")\n",
    "print(f\"   ‚úì 60% less memory usage\")\n",
    "print(f\"   ‚úì Same accuracy as full precision training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f291857",
   "metadata": {
    "id": "8f291857"
   },
   "source": [
    "## Step 7: Test the DPO-Trained Model\n",
    "\n",
    "Now let's test if the model learned to prefer better responses!\n",
    "\n",
    "We'll:\n",
    "1. Give the model a prompt\n",
    "2. Generate a response\n",
    "3. Compare with the original model's behavior (conceptually)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1929439d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1929439d",
    "outputId": "f95e3683-be56-4354-d1fa-d486daefab06"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing DPO-Trained Model\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "User: Explain the concept of machine learning in simple terms that a beginner can understand.\n",
      "\n",
      "================================================================================\n",
      "MODEL RESPONSE:\n",
      "--------------------------------------------------------------------------------\n",
      "<|im_end|>\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextStreamer\n",
    "\n",
    "# Enable fast inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "print(\"üß™ Testing DPO-Trained Model\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test prompt\n",
    "test_prompt = \"\"\"User: Explain the concept of machine learning in simple terms that a beginner can understand.\"\"\"\n",
    "\n",
    "# Tokenize the prompt\n",
    "inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"PROMPT:\\n{test_prompt}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "# Generate response with streaming\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8b21d0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ec8b21d0",
    "outputId": "23abd5b5-b4bd-497f-eb17-cf98cdb79859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "================================================================================\n",
      "PROMPT:\n",
      "User: Write a short Python function to calculate factorial.\n",
      "\n",
      "================================================================================\n",
      "MODEL RESPONSE:\n",
      "--------------------------------------------------------------------------------\n",
      "<|im_end|>\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with another prompt\n",
    "print(\"\\n\\n\" + \"=\"*80)\n",
    "test_prompt_2 = \"\"\"User: Write a short Python function to calculate factorial.\"\"\"\n",
    "\n",
    "inputs = tokenizer(test_prompt_2, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "print(f\"PROMPT:\\n{test_prompt_2}\\n\")\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL RESPONSE:\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    streamer=text_streamer,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    use_cache=True,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382e521f",
   "metadata": {
    "id": "382e521f"
   },
   "source": [
    "## Step 8: Save the Fine-tuned Model\n",
    "\n",
    "Let's save our DPO-trained model so we can use it later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81282f76",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "81282f76",
    "outputId": "e50c8d1f-8217-49e7-a41a-707f8c58f833"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saving DPO-trained model to ./smollm2_dpo_model...\n",
      "‚úÖ Model saved successfully!\n",
      "   Location: ./smollm2_dpo_model\n",
      "   Files saved: adapter_config.json, adapter_model.safetensors, tokenizer files\n",
      "\n",
      "üîÄ You can also merge LoRA adapters with base model:\n",
      "   model.save_pretrained_merged('./smollm2_dpo_model_merged', tokenizer)\n",
      "   This creates a single model file without adapters.\n"
     ]
    }
   ],
   "source": [
    "# Save the model locally\n",
    "model_save_path = \"./smollm2_dpo_model\"\n",
    "\n",
    "print(f\"üíæ Saving DPO-trained model to {model_save_path}...\")\n",
    "\n",
    "# Save LoRA adapters\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"‚úÖ Model saved successfully!\")\n",
    "print(f\"   Location: {model_save_path}\")\n",
    "print(f\"   Files saved: adapter_config.json, adapter_model.safetensors, tokenizer files\")\n",
    "\n",
    "# Optional: Merge LoRA adapters with base model for easier deployment\n",
    "print(f\"\\nüîÄ You can also merge LoRA adapters with base model:\")\n",
    "print(f\"   model.save_pretrained_merged('{model_save_path}_merged', tokenizer)\")\n",
    "print(f\"   This creates a single model file without adapters.\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "include_colab_link": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
