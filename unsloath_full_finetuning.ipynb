{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ykalathiya-2/unsloath/blob/main/unsloath_full_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VGJcnKvw84Iy"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "!pip install --upgrade -qqq uv\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    # If you're not in Colab, just use pip install!\n",
        "    !pip install unsloth vllm synthetic-data-kit==0.0.3\n",
        "else:\n",
        "    try: import numpy, PIL; get_numpy = f\"numpy=={numpy.__version__}\"; get_pil = f\"pillow=={PIL.__version__}\"\n",
        "    except: get_numpy = \"numpy\"; get_pil = \"pillow\"\n",
        "    try: import subprocess; is_t4 = \"Tesla T4\" in str(subprocess.check_output([\"nvidia-smi\"]))\n",
        "    except: is_t4 = False\n",
        "    get_vllm, get_triton = (\"vllm==0.9.2\", \"triton==3.2.0\") if is_t4 else (\"vllm==0.10.2\", \"triton\")\n",
        "    !uv pip install -qqq --upgrade         unsloth {get_vllm} {get_numpy} {get_pil} torchvision bitsandbytes xformers\n",
        "    !uv pip install -qqq {get_triton}\n",
        "    !uv pip install synthetic-data-kit==0.0.3\n",
        "!uv pip install transformers==4.56.2\n",
        "!uv pip install --no-deps trl==0.22.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61CD9Psm9IEJ",
        "outputId": "25779f68-6dac-44ab-be8f-937ea14edee9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO 11-10 03:48:27 [vllm_utils.py:700] Unsloth: Patching vLLM v1 graph capture\n",
            "INFO 11-10 03:48:27 [vllm_utils.py:730] Unsloth: Patching vLLM v0 graph capture\n",
            "Unsloth: Using dtype = torch.float16 for vLLM.\n",
            "Unsloth: vLLM loading unsloth/Llama-3.2-3B-Instruct with actual GPU utilization = 78.21%\n",
            "Unsloth: Your GPU has CUDA compute capability 7.5 with VRAM = 14.74 GB.\n",
            "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 192.\n",
            "Unsloth: vLLM's KV Cache can use up to 5.55 GB. Also swap space = 4 GB.\n",
            "Unsloth: `cudagraph_mode` is not in `from vllm.config import CompilationConfig`\n",
            "Unsloth: Disabling `disable_cascade_attn` in vLLM to allow for better on policy RL!\n",
            "vLLM STDOUT: INFO 11-10 03:48:39 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 11-10 03:48:41 [api_server.py:1395] vLLM API server version 0.9.2\n",
            "vLLM STDOUT: INFO 11-10 03:48:41 [cli_args.py:325] non-default args: {'model': 'unsloth/Llama-3.2-3B-Instruct', 'dtype': 'float16', 'seed': 0, 'max_model_len': 2048, 'max_logprobs': 0, 'disable_cascade_attn': True, 'gpu_memory_utilization': 0.7821307463947235, 'enable_prefix_caching': True, 'max_num_batched_tokens': 2048, 'max_num_seqs': 192, 'enable_chunked_prefill': True, 'disable_log_stats': True}\n",
            "vLLM STDOUT: INFO 11-10 03:48:56 [config.py:841] This model supports multiple tasks: {'embed', 'generate', 'reward', 'classify'}. Defaulting to 'generate'.\n",
            "vLLM STDOUT: WARNING 11-10 03:48:56 [config.py:3371] Casting torch.bfloat16 to torch.float16.\n",
            "vLLM STDOUT: INFO 11-10 03:48:56 [config.py:1472] Using max model len 2048\n",
            "vLLM STDOUT: WARNING 11-10 03:48:56 [arg_utils.py:1735] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. \n",
            "vLLM STDOUT: INFO 11-10 03:48:57 [config.py:2285] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
            "vLLM STDOUT: INFO 11-10 03:48:57 [api_server.py:268] Started engine process with PID 11876\n",
            "vLLM STDOUT: INFO 11-10 03:49:10 [__init__.py:244] Automatically detected platform cuda.\n",
            "vLLM STDOUT: INFO 11-10 03:49:11 [llm_engine.py:230] Initializing a V0 LLM engine (v0.9.2) with config: model='unsloth/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='unsloth/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config={}, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=unsloth/Llama-3.2-3B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":0,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":0,\"cudagraph_capture_sizes\":[192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"max_capture_size\":192,\"local_cache_dir\":null}, use_cached_outputs=True, \n",
            "vLLM STDOUT: INFO 11-10 03:49:12 [cuda.py:311] Cannot use FlashAttention-2 backend for Volta and Turing GPUs.\n",
            "vLLM STDOUT: INFO 11-10 03:49:12 [cuda.py:360] Using XFormers backend.\n",
            "vLLM STDOUT: INFO 11-10 03:49:13 [parallel_state.py:1076] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
            "vLLM STDOUT: INFO 11-10 03:49:13 [model_runner.py:1171] Starting to load model unsloth/Llama-3.2-3B-Instruct...\n",
            "vLLM STDOUT: INFO 11-10 03:49:14 [weight_utils.py:292] Using model weights format ['*.safetensors']\n",
            "vLLM STDOUT: INFO 11-10 03:49:18 [default_loader.py:272] Loading weights took 3.68 seconds\n",
            "vLLM STDOUT: INFO 11-10 03:49:19 [model_runner.py:1203] Model loading took 6.0160 GiB and 4.148048 seconds\n",
            "vLLM STDOUT: INFO 11-10 03:49:20 [worker.py:294] Memory profiling takes 1.19 seconds\n",
            "vLLM STDOUT: INFO 11-10 03:49:20 [worker.py:294] the current vLLM instance can use total_gpu_memory (14.74GiB) x gpu_memory_utilization (0.78) = 11.53GiB\n",
            "vLLM STDOUT: INFO 11-10 03:49:20 [worker.py:294] model weights take 6.02GiB; non_torch_memory takes 0.05GiB; PyTorch activation peak memory takes 0.90GiB; the rest of the memory reserved for KV Cache is 4.57GiB.\n",
            "vLLM STDOUT: INFO 11-10 03:49:21 [executor_base.py:113] # cuda blocks: 2673, # CPU blocks: 2340\n",
            "vLLM STDOUT: INFO 11-10 03:49:21 [executor_base.py:118] Maximum concurrency for 2048 tokens per request: 20.88x\n",
            "vLLM STDOUT: INFO 11-10 03:49:24 [model_runner.py:1513] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
            "vLLM STDOUT: INFO 11-10 03:49:48 [model_runner.py:1671] Graph capturing finished in 25 secs, took 0.15 GiB\n",
            "vLLM STDOUT: INFO 11-10 03:49:48 [llm_engine.py:428] init engine (profile, create kv cache, warmup model) took 29.92 seconds\n",
            "vLLM STDOUT: WARNING 11-10 03:49:49 [config.py:1392] Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [serving_chat.py:125] Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [serving_completion.py:72] Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [api_server.py:1457] Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:29] Available routes are:\n",
            "vLLM Server Ready Detected\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /openapi.json, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /docs, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /docs/oauth2-redirect, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /redoc, Methods: GET, HEAD\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /health, Methods: GET\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /load, Methods: GET\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /ping, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /ping, Methods: GET\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /tokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /detokenize, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/models, Methods: GET\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /version, Methods: GET\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/chat/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/completions, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/embeddings, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /pooling, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /classify, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /score, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/score, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/audio/transcriptions, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/audio/translations, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v1/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /v2/rerank, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /invocations, Methods: POST\n",
            "vLLM STDOUT: INFO 11-10 03:49:49 [launcher.py:37] Route: /metrics, Methods: GET\n",
            "vLLM STDOUT: INFO:     127.0.0.1:35684 - \"GET /metrics HTTP/1.1\" 200 OK\n",
            "Attempting to terminate the VLLM server gracefully...\n",
            "Server terminated gracefully.\n"
          ]
        }
      ],
      "source": [
        "from unsloth.dataprep import SyntheticDataKit\n",
        "\n",
        "torch_dtype = torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32\n",
        "generator = SyntheticDataKit.from_pretrained(\n",
        "    # Choose any model from https://huggingface.co/unsloth\n",
        "    model_name = \"unsloth/Llama-3.2-3B-Instruct\",\n",
        "    max_seq_length = 2048, # Longer sequence lengths will be slower!\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-Cf_Thi9UJe"
      },
      "source": [
        "## Generate QA Pairs + Auto clean data\n",
        "We now use synthetic data kit for question answer pair generation:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Rh5mLZLu9SG0"
      },
      "outputs": [],
      "source": [
        "generator.prepare_qa_generation(\n",
        "    output_folder = \"data\", # Output location of synthetic data\n",
        "    temperature = 0.7, # Higher temp makes more diverse datases\n",
        "    top_p = 0.95,\n",
        "    overlap = 64, # Overlap portion during chunking\n",
        "    max_generation_tokens = 512, # Can increase for longer QA pairs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pybVCdFl9xS6"
      },
      "source": [
        "Check if it succeeded:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V3u45wXp9WkD",
        "outputId": "bc2ebd71-c752-41c3-8ae4-4b9a5cc07e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:59156 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25l\u001b[32m VLLM server is running at \u001b[0m\u001b[4;94mhttp://localhost:8000/v1\u001b[0m\n",
            "\u001b[32m‚†ã\u001b[0m\u001b[32m Checking VLLM server at http://localhost:8000/v1...\u001b[0m\r\u001b[2KAvailable models: \u001b[1m{\u001b[0m\u001b[32m'object'\u001b[0m: \u001b[32m'list'\u001b[0m, \u001b[32m'data'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model'\u001b[0m, \u001b[32m'created'\u001b[0m: \u001b[1;36m1762745765\u001b[0m, \n",
            "\u001b[32m'owned_by'\u001b[0m: \u001b[32m'vllm'\u001b[0m, \u001b[32m'root'\u001b[0m: \u001b[32m'unsloth/Llama-3.2-3B-Instruct'\u001b[0m, \u001b[32m'parent'\u001b[0m: \u001b[3;35mNone\u001b[0m, \n",
            "\u001b[32m'max_model_len'\u001b[0m: \u001b[1;36m2048\u001b[0m, \u001b[32m'permission'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m{\u001b[0m\u001b[32m'id'\u001b[0m: \n",
            "\u001b[32m'modelperm-65276389bc76471697bf6e384f4faa19'\u001b[0m, \u001b[32m'object'\u001b[0m: \u001b[32m'model_permission'\u001b[0m, \n",
            "\u001b[32m'created'\u001b[0m: \u001b[1;36m1762745765\u001b[0m, \u001b[32m'allow_create_engine'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_sampling'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_logprobs'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \u001b[32m'allow_search_indices'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'allow_view'\u001b[0m: \u001b[3;92mTrue\u001b[0m, \n",
            "\u001b[32m'allow_fine_tuning'\u001b[0m: \u001b[3;91mFalse\u001b[0m, \u001b[32m'organization'\u001b[0m: \u001b[32m'*'\u001b[0m, \u001b[32m'group'\u001b[0m: \u001b[3;35mNone\u001b[0m, \u001b[32m'is_blocking'\u001b[0m: \n",
            "\u001b[3;91mFalse\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\u001b[1m]\u001b[0m\u001b[1m}\u001b[0m\n",
            "\u001b[32m‚†ã\u001b[0m Checking VLLM server at http://localhost:8000/v1...\r\u001b[2K\u001b[32m‚†ã\u001b[0m Checking VLLM server at http://localhost:8000/v1...\n",
            "\u001b[?25h\r\u001b[1A\u001b[2K"
          ]
        }
      ],
      "source": [
        "!synthetic-data-kit system-check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaLT122S907a"
      },
      "source": [
        "## Document Parsing (PDF, CSV, HTML etc.)\n",
        "Now, let's take the Byte Latent Transformer: Patches Scale Better Than Tokens research paper found at https://arxiv.org/abs/2412.09871 and covert it to Q&A pairs in order to finetune Llama 3.2!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mhtUTqVj9YiB",
        "outputId": "82e40b63-bf50-45fb-c249-60162ba740b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[32m‚†è\u001b[0m Processing https://arxiv.org/html/2412.09871v1...\n",
            "\u001b[1A\u001b[2K\u001b[32m Text successfully extracted to \u001b[0m\u001b[1;32mdata/output/arxiv_org.txt\u001b[0m\n",
            "34 ['data/output/arxiv_org_0.txt', 'data/output/arxiv_org_1.txt', 'data/output/arxiv_org_2.txt']\n"
          ]
        }
      ],
      "source": [
        "# Byte Latent Transformer: Patches Scale Better Than Tokens paper in HTML format\n",
        "!synthetic-data-kit \\\n",
        "    -c synthetic_data_kit_config.yaml \\\n",
        "    ingest \"https://arxiv.org/html/2412.09871v1\"\n",
        "\n",
        "# Truncate document\n",
        "filenames = generator.chunk_data(\"data/output/arxiv_org.txt\")\n",
        "print(len(filenames), filenames[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoTHBFwW93hd"
      },
      "source": [
        "We see around 37 chunks of data. We now call synthetic-data-kit to create some pairs of data for 3 of our chunks.\n",
        "\n",
        "You can process more chunks, but it'll be much slower!\n",
        "\n",
        "Using `--num-pairs` will generate **approximately** that many QA pairs. However it might be shorter or longer depending on the `max_seq_length` of the loaded up model. So if you specify 100, you might only get 10 since the model's max sequence length is capped."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NxDPSin9oVJ",
        "outputId": "52c936f5-33a1-48ad-ba14-3d635949bcd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vLLM STDOUT: INFO:     127.0.0.1:42442 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[?25lvLLM STDOUT: INFO:     127.0.0.1:42446 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "\u001b[2K\u001b[32m‚†¶\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO 11-10 03:36:08 [chat_utils.py:444] Detected the chat template content format to be 'string'. You can set `--chat-template-content-format` to override this.\n",
            "vLLM STDOUT: INFO 11-10 03:36:08 [logger.py:43] Received request chatcmpl-ed88e9df0a32425a90db8933f5d3db45: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nByte Latent Transformer: Patches Scale Better Than Tokens\\n1 Introduction\\n2 Patching: From Individual Bytes to Groups of Bytes\\n2.1 Strided Patching Every K Bytes\\n2.2 Space Patching\\n2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM\\n2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching\\n3 BLT Architecture\\n3.1 Latent Global Transformer Model\\n3.2 Local Encoder\\n3.2.1 Encoder Hash n-gram Embeddings\\n3.2.2 Encoder Multi-Headed Cross-Attention\\n3.3 Local Decoder\\n3.3.1 Decoder Multi-headed Cross-Attention\\n4 Experimental Setup\\n4.1 Pre-training Datasets\\n4.2 Entropy Model\\n4.3 Entropy Threshold and Equalizing Context Length\\n4.4 Entropy Model Context\\n4.5 FLOPs Estimation\\n4.6 Bits-Per-Byte Estimation\\n4.7 Transformer Architecture Hyperparameters\\n4.8 BLT-Specific Hyperparameters\\n5 Scaling Trends\\n5.1 Parameter Matched Compute Optimal Scaling Trends\\n5.2 Beyond Compute Optimal Task Evaluations\\nClassification tasks\\nCoding related generation tasks:\\n5.3 Patches Scale Better Than Tokens\\n6 Byte Modeling Improves Robustness\\n6.1 Character-Level Tasks\\nNoisy Data\\nPhonology - Grapheme-to-Phoneme (G2P)\\nCUTE\\nLow Resource Machine Translation\\n6.2 Training BLT from Llama 3\\n7 Ablations and Discussion\\nEntropy Model Hyper-parameters\\nTypes of Patching\\nCross-Attention\\nn-gram Hash Embeddings\\nLocal Model Hyperparamaters\\n8 Related Work\\n9 Limitations and Future Work\\n10 Conclusion\\nCore Contributors:\\nCore Advising Group:\\nAdvisors and Contributors:\\n11 Model Hyper Parameters\\n12 FLOPs Equations\\n13 Rolling Polynomial Hashing\\n14 Frequency-based n-gram Embedddings\\n15 Entropy Patching Example from MMLU\\n]FAIR at Meta\\n1]Paul G. Allen School of Computer Science & Engineering, University of Washington\\n2]University of Chicago\\n\\\\contribution\\n[‚Ä°]Joint second author\\n\\\\contribution[‚Ä†]Joint last author\\n\\\\contribution[‚ãÑ]Work done at Meta\\nByte Latent Transformer: Patches Scale Better Than Tokens\\nArtidoro Pagnoni\\nRam Pasunuru\\nPedro Rodriguez\\nJohn Nguyen\\nBenjamin Muller\\nMargaret Li\\nChunting Zhou\\nLili Yu\\nJason Weston\\nLuke Zettlemoyer\\nGargi Ghosh\\nMike Lewis\\nAri Holtzman\\nSrinivasan Iyer\\n[\\n[\\n[\\ncs.washington.edu\\nmeta.com\\n(July 25, 2025)\\nAbstract\\nWe introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.\\nBLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.\\nPatches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it.\\nWe present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary.\\nBoth training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.\\n\\\\correspondence\\nartidoro at, sviyer at\\n\\\\metadata\\n[Code]https://github.com/facebookresearch/blt\\n1 Introduction\\nFigure 1:\\nScaling trends for fixed inference flop models (fully) trained with varying training budgets.\\nIn token-based models, a fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 11-10 03:36:08 [engine.py:317] Added request chatcmpl-ed88e9df0a32425a90db8933f5d3db45.\n",
            "\u001b[2K\u001b[32m‚†ô\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO:     127.0.0.1:42460 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[32m‚†π\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO 11-10 03:36:24 [logger.py:43] Received request chatcmpl-12d9602dd43843baaaf07787c93ebe3c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\nByte Latent Transformer: Patches Scale Better Than Tokens\\n1 Introduction\\n2 Patching: From Individual Bytes to Groups of Bytes\\n2.1 Strided Patching Every K Bytes\\n2.2 Space Patching\\n2.3 Entropy Patching: Using Next-Byte Entropies from a Small Byte LM\\n2.4 The Byte-Pair Encoding (BPE) Tokenizer and Incremental Patching\\n3 BLT Architecture\\n3.1 Latent Global Transformer Model\\n3.2 Local Encoder\\n3.2.1 Encoder Hash n-gram Embeddings\\n3.2.2 Encoder Multi-Headed Cross-Attention\\n3.3 Local Decoder\\n3.3.1 Decoder Multi-headed Cross-Attention\\n4 Experimental Setup\\n4.1 Pre-training Datasets\\n4.2 Entropy Model\\n4.3 Entropy Threshold and Equalizing Context Length\\n4.4 Entropy Model Context\\n4.5 FLOPs Estimation\\n4.6 Bits-Per-Byte Estimation\\n4.7 Transformer Architecture Hyperparameters\\n4.8 BLT-Specific Hyperparameters\\n5 Scaling Trends\\n5.1 Parameter Matched Compute Optimal Scaling Trends\\n5.2 Beyond Compute Optimal Task Evaluations\\nClassification tasks\\nCoding related generation tasks:\\n5.3 Patches Scale Better Than Tokens\\n6 Byte Modeling Improves Robustness\\n6.1 Character-Level Tasks\\nNoisy Data\\nPhonology - Grapheme-to-Phoneme (G2P)\\nCUTE\\nLow Resource Machine Translation\\n6.2 Training BLT from Llama 3\\n7 Ablations and Discussion\\nEntropy Model Hyper-parameters\\nTypes of Patching\\nCross-Attention\\nn-gram Hash Embeddings\\nLocal Model Hyperparamaters\\n8 Related Work\\n9 Limitations and Future Work\\n10 Conclusion\\nCore Contributors:\\nCore Advising Group:\\nAdvisors and Contributors:\\n11 Model Hyper Parameters\\n12 FLOPs Equations\\n13 Rolling Polynomial Hashing\\n14 Frequency-based n-gram Embedddings\\n15 Entropy Patching Example from MMLU\\n]FAIR at Meta\\n1]Paul G. Allen School of Computer Science & Engineering, University of Washington\\n2]University of Chicago\\n\\\\contribution\\n[‚Ä°]Joint second author\\n\\\\contribution[‚Ä†]Joint last author\\n\\\\contribution[‚ãÑ]Work done at Meta\\nByte Latent Transformer: Patches Scale Better Than Tokens\\nArtidoro Pagnoni\\nRam Pasunuru\\nPedro Rodriguez\\nJohn Nguyen\\nBenjamin Muller\\nMargaret Li\\nChunting Zhou\\nLili Yu\\nJason Weston\\nLuke Zettlemoyer\\nGargi Ghosh\\nMike Lewis\\nAri Holtzman\\nSrinivasan Iyer\\n[\\n[\\n[\\ncs.washington.edu\\nmeta.com\\n(July 25, 2025)\\nAbstract\\nWe introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.\\nBLT encodes bytes into dynamically sized patches, which serve as the primary units of computation.\\nPatches are segmented based on the entropy of the next byte, allocating more compute and model capacity where increased data complexity demands it.\\nWe present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary.\\nBoth training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.\\n\\\\correspondence\\nartidoro at, sviyer at\\n\\\\metadata\\n[Code]https://github.com/facebookresearch/blt\\n1 Introduction\\nFigure 1:\\nScaling trends for fixed inference flop models (fully) trained with varying training budgets.\\nIn token-based models, a fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 11-10 03:36:24 [engine.py:317] Added request chatcmpl-12d9602dd43843baaaf07787c93ebe3c.\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...vLLM STDOUT: INFO:     127.0.0.1:50430 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 10 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_0_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_0_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Generating qa content from data/output/arxiv_org_0.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_0_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45414 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:45416 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 11-10 03:36:41 [logger.py:43] Received request chatcmpl-9c795df4d5f64f85b5b0a46e6f96df74: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\na fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.\\nFigure 2:\\nBLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. BLT incorporates byte nnitalic_n-gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modules\\xa0(Figure\\xa05). Unlike fixed-vocabulary tokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.\\nWe introduce the Byte Latent Transformer\\xa0(BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (¬ß6).\\nExisting large language models (llms) are trained almost entirely end-to-end, except for tokenization‚Äîa heuristic pre-processing step that groups bytes into a static set of tokens.\\nSuch tokens bias how a string is compressed, leading to shortcomings such as domain/modality sensitivity\\xa0(Dagan et\\xa0al., 2024), sensitivity to input noise\\xa0(¬ß6), a lack of orthographic knowledge\\xa0(Edman et\\xa0al., 2024), and multilingual inequity\\xa0(Liang et\\xa0al., 2023; Petrov et\\xa0al., 2024; Limisiewicz et\\xa0al., 2024).\\nTokenization has previously been essential because directly training llms on bytes is prohibitively costly at scale due to long sequence lengths\\xa0(Xue et\\xa0al., 2022).\\nPrior works mitigate this by employing more efficient self-attention\\xa0(El\\xa0Boukkouri et\\xa0al., 2020; Clark et\\xa0al., 2022) or attention-free architectures\\xa0(Wang et\\xa0al., 2024)\\xa0(¬ß8). However, this primarily helps train small models.\\nAt scale, the computational cost of a Transformer is dominated by large feed-forward network layers that run on every byte, not the cost of the attention mechanism.\\nTo efficiently allocate compute, we propose a dynamic, learnable method for grouping bytes into patches\\xa0(¬ß2) and a new model architecture that mixes byte and patch information.\\nUnlike tokenization, BLT has no fixed vocabulary for patches.\\nArbitrary groups of bytes are mapped to latent patch representations via light-weight learned encoder and decoder modules.\\nWe show that this results in more efficient allocation of compute than tokenization-based models.\\nTokenization-based llms allocate the same amount of compute to every token. This trades efficiency for performance, since tokens are induced with compression heuristics that are not always correlated with the complexity of predictions. Central to our architecture is the idea that models should dynamically allocate compute where it is needed. For example, a large transformer is not needed to predict the ending of most words, since these are comparably easy, low-entropy decisions compared to choosing the first word of a new sentence.\\nThis is reflected in BLT‚Äôs architecture\\xa0(¬ß3) where there are three transformer blocks: two small byte-level local models and a large global latent transformer\\xa0(Figure\\xa02).\\nTo determine how to group bytes into patches and therefore how to dynamically allocate compute, BLT\\nsegments data based on the entropy of the next-byte prediction creating contextualized groupings of bytes with relatively uniform information density.\\nWe present the first flop-controlled scaling study of byte-level models up to 8B parameters and 4T training bytes, showing that we can train a model end-to-end at scale from bytes without fixed-vocabulary tokenization.\\nOverall, BLT matches training flop-controlled performance111We calculate the computational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 11-10 03:36:41 [engine.py:317] Added request chatcmpl-9c795df4d5f64f85b5b0a46e6f96df74.\n",
            "\u001b[2K\u001b[32m‚†∏\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...vLLM STDOUT: INFO:     127.0.0.1:45430 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 11-10 03:36:46 [logger.py:43] Received request chatcmpl-f8d579628fcf407088ddb27a9944644c: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n a fixed inference budget determines the model size.\\nIn contrast, the BLT architecture provides a new scaling axis allowing simultaneous increases in model and patch size while keeping the same training and inference budget.\\nBLT patch-size (ps) 6 and 8 models quickly overtake scaling trends of bpe Llama\\xa02 and 3. Moving to the larger inference budget makes the larger patch size 8 model more desirable sooner. Both BPE compute-optimal point and crossover point are indicated with vertical lines.\\nFigure 2:\\nBLT comprises three modules, a lightweight Local Encoder that encodes input bytes into patch representations, a computationally expensive Latent Transformer over patch representations, and a lightweight Local Decoder to decode the next patch of bytes. BLT incorporates byte nnitalic_n-gram embeddings and a cross-attention mechanism to maximize information flow between the Latent Transformer and the byte-level modules\\xa0(Figure\\xa05). Unlike fixed-vocabulary tokenization, BLT dynamically groups bytes into patches preserving access to the byte-level information.\\nWe introduce the Byte Latent Transformer\\xa0(BLT), a tokenizer-free architecture that learns from raw byte data and, for the first time, matches the performance of tokenization-based models at scale, with significant improvements in efficiency and robustness (¬ß6).\\nExisting large language models (llms) are trained almost entirely end-to-end, except for tokenization‚Äîa heuristic pre-processing step that groups bytes into a static set of tokens.\\nSuch tokens bias how a string is compressed, leading to shortcomings such as domain/modality sensitivity\\xa0(Dagan et\\xa0al., 2024), sensitivity to input noise\\xa0(¬ß6), a lack of orthographic knowledge\\xa0(Edman et\\xa0al., 2024), and multilingual inequity\\xa0(Liang et\\xa0al., 2023; Petrov et\\xa0al., 2024; Limisiewicz et\\xa0al., 2024).\\nTokenization has previously been essential because directly training llms on bytes is prohibitively costly at scale due to long sequence lengths\\xa0(Xue et\\xa0al., 2022).\\nPrior works mitigate this by employing more efficient self-attention\\xa0(El\\xa0Boukkouri et\\xa0al., 2020; Clark et\\xa0al., 2022) or attention-free architectures\\xa0(Wang et\\xa0al., 2024)\\xa0(¬ß8). However, this primarily helps train small models.\\nAt scale, the computational cost of a Transformer is dominated by large feed-forward network layers that run on every byte, not the cost of the attention mechanism.\\nTo efficiently allocate compute, we propose a dynamic, learnable method for grouping bytes into patches\\xa0(¬ß2) and a new model architecture that mixes byte and patch information.\\nUnlike tokenization, BLT has no fixed vocabulary for patches.\\nArbitrary groups of bytes are mapped to latent patch representations via light-weight learned encoder and decoder modules.\\nWe show that this results in more efficient allocation of compute than tokenization-based models.\\nTokenization-based llms allocate the same amount of compute to every token. This trades efficiency for performance, since tokens are induced with compression heuristics that are not always correlated with the complexity of predictions. Central to our architecture is the idea that models should dynamically allocate compute where it is needed. For example, a large transformer is not needed to predict the ending of most words, since these are comparably easy, low-entropy decisions compared to choosing the first word of a new sentence.\\nThis is reflected in BLT‚Äôs architecture\\xa0(¬ß3) where there are three transformer blocks: two small byte-level local models and a large global latent transformer\\xa0(Figure\\xa02).\\nTo determine how to group bytes into patches and therefore how to dynamically allocate compute, BLT\\nsegments data based on the entropy of the next-byte prediction creating contextualized groupings of bytes with relatively uniform information density.\\nWe present the first flop-controlled scaling study of byte-level models up to 8B parameters and 4T training bytes, showing that we can train a model end-to-end at scale from bytes without fixed-vocabulary tokenization.\\nOverall, BLT matches training flop-controlled performance111We calculate the computational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 11-10 03:36:46 [engine.py:317] Added request chatcmpl-f8d579628fcf407088ddb27a9944644c.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†π\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...vLLM STDOUT: INFO:     127.0.0.1:45432 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 13 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_1_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_1_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/arxiv_org_1.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_1_qa_pairs.json\u001b[0m\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40094 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO:     127.0.0.1:40098 - \"GET /v1/models HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 11-10 03:37:03 [logger.py:43] Received request chatcmpl-d4a774ee8b2e42609342ee44be8862d0: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nSummarize this document in 3-5 sentences, focusing on the main topic and key concepts.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\ncomputational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data. BLT models are more robust than tokenizer-based models to noisy inputs and display enhanced character level understanding abilities demonstrated on orthographic knowledge, phonology, and low-resource machine translation tasks\\xa0(¬ß6).\\nFinally, with BLT models, we can simultaneously increase model size and patch size while maintaining the same inference flop budget. Longer patch sizes, on average, save compute which can be reallocated to grow the size of the global latent transformer, because it is run less often. We conduct inference-flop controlled scaling experiments\\xa0(Figure\\xa01), and observe significantly better scaling trends than with tokenization-based architectures.\\nIn summary, this paper makes the following contributions:\\n1) We introduce BLT, a byte latent llm architecture that dynamically allocates compute to improve flop efficiency,\\n2) We show that we achieve training flop-controlled parity with Llama 3 up to 8B scale while having the option to trade minor losses in evaluation metrics for flop efficiency gains of up to 50%, 3) BLT models unlock a new dimension for scaling llms, where model size can now be scaled while maintaining a fixed-inference budget, 4) We demonstrate the improved robustness of BLT models to input noise and their awareness of sub-word aspects of input data that token-based llms miss.\\nWe release the training and inference code for BLT at\\xa0https://github.com/facebookresearch/blt.\\n2 Patching: From Individual Bytes to Groups of Bytes\\nFigure 3:\\nPatching schemes group bytes in different ways, each leading to a different number of resulting patches.\\nSince each patch is processed using a large transformer step, the number of patches directly determines the bulk of the compute expended in terms of flops.\\nThese schemes group bytes into patches by (a) striding every four bytes\\xa0(¬ß2.1) as in MegaByte\\xa0(Yu et\\xa0al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3\\xa0(Dubey et\\xa0al., 2024) tokenizer, (c & d) entropy-based patching as in this work\\xa0(¬ß2.3), (e) patching on space-bytes\\xa0(Slagle, 2024), (f) and patching on entropy using a small CNN byte-level model with 2-byte context.\\nSegmenting bytes into patches allows BLT to dynamically allocate compute based on context.\\nFigure\\xa03 shows several different methods for segmenting bytes into patches.\\nFormally, a patching function fpf_{p}italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT segments a sequence of bytes ùíô={xi,|i=1,‚Ä¶n}\\\\boldsymbol{x}=\\\\{x_{i},|i=1,\\\\ldots n\\\\}bold_italic_x = { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, | italic_i = 1, ‚Ä¶ italic_n } of length nnitalic_n into a sequence of m<nm<nitalic_m < italic_n patches ùíë={pj|j=1,‚Ä¶,m}\\\\boldsymbol{p}=\\\\{p_{j}|j=1,\\\\ldots,m\\\\}bold_italic_p = { italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_j = 1, ‚Ä¶, italic_m } by mapping each xix_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the set {0,1} where 1 indicates the start of a new patch.\\nFor both token-based and patch-based models, the computational cost of processing data is primarily determined by the number of steps executed by the main Transformer. In BLT, this is the number of patches needed to encode the data with a given patching function.\\nConsequently, the average size of a patch, or simply patch size, is the main factor for determining the cost of processing data during both training and inference with a given patching function\\xa0(¬ß4.5).\\nNext, we introduce three patching functions: patching with a fixed number of bytes per patch\\xa0(¬ß2<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.1, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 11-10 03:37:03 [engine.py:317] Added request chatcmpl-d4a774ee8b2e42609342ee44be8862d0.\n",
            "\u001b[2K\u001b[32m‚†¥\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...vLLM STDOUT: INFO:     127.0.0.1:40100 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "vLLM STDOUT: INFO 11-10 03:37:08 [logger.py:43] Received request chatcmpl-6498cd60203346659477dceba3210222: prompt: '<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 10 Nov 2025\\n\\nCreate 25 question-answer pairs from this text for LLM training.\\n\\nRules:\\n1. Questions must be about important facts in the text\\n2. Answers must be directly supported by the text\\n3. Return JSON format only:\\n\\n[\\n  {\\n    \"question\": \"Question 1?\",\\n    \"answer\": \"Answer 1.\"\\n  },\\n  {\\n    \"question\": \"Question 2?\",\\n    \"answer\": \"Answer 2.\"\\n  }\\n]\\n\\nText:\\n computational cost of a model by counting the number of Floating Point OPerations (flops) needed. of Llama 3 while using up to 50% fewer flops at inference\\xa0(¬ß5).\\nWe also show that directly working with raw bytes provides significant improvements in modeling the long-tail of the data. BLT models are more robust than tokenizer-based models to noisy inputs and display enhanced character level understanding abilities demonstrated on orthographic knowledge, phonology, and low-resource machine translation tasks\\xa0(¬ß6).\\nFinally, with BLT models, we can simultaneously increase model size and patch size while maintaining the same inference flop budget. Longer patch sizes, on average, save compute which can be reallocated to grow the size of the global latent transformer, because it is run less often. We conduct inference-flop controlled scaling experiments\\xa0(Figure\\xa01), and observe significantly better scaling trends than with tokenization-based architectures.\\nIn summary, this paper makes the following contributions:\\n1) We introduce BLT, a byte latent llm architecture that dynamically allocates compute to improve flop efficiency,\\n2) We show that we achieve training flop-controlled parity with Llama 3 up to 8B scale while having the option to trade minor losses in evaluation metrics for flop efficiency gains of up to 50%, 3) BLT models unlock a new dimension for scaling llms, where model size can now be scaled while maintaining a fixed-inference budget, 4) We demonstrate the improved robustness of BLT models to input noise and their awareness of sub-word aspects of input data that token-based llms miss.\\nWe release the training and inference code for BLT at\\xa0https://github.com/facebookresearch/blt.\\n2 Patching: From Individual Bytes to Groups of Bytes\\nFigure 3:\\nPatching schemes group bytes in different ways, each leading to a different number of resulting patches.\\nSince each patch is processed using a large transformer step, the number of patches directly determines the bulk of the compute expended in terms of flops.\\nThese schemes group bytes into patches by (a) striding every four bytes\\xa0(¬ß2.1) as in MegaByte\\xa0(Yu et\\xa0al., 2023), (b) tokenizing with Byte-Pair Encoding (bpe), in this case the Llama-3\\xa0(Dubey et\\xa0al., 2024) tokenizer, (c & d) entropy-based patching as in this work\\xa0(¬ß2.3), (e) patching on space-bytes\\xa0(Slagle, 2024), (f) and patching on entropy using a small CNN byte-level model with 2-byte context.\\nSegmenting bytes into patches allows BLT to dynamically allocate compute based on context.\\nFigure\\xa03 shows several different methods for segmenting bytes into patches.\\nFormally, a patching function fpf_{p}italic_f start_POSTSUBSCRIPT italic_p end_POSTSUBSCRIPT segments a sequence of bytes ùíô={xi,|i=1,‚Ä¶n}\\\\boldsymbol{x}=\\\\{x_{i},|i=1,\\\\ldots n\\\\}bold_italic_x = { italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT, | italic_i = 1, ‚Ä¶ italic_n } of length nnitalic_n into a sequence of m<nm<nitalic_m < italic_n patches ùíë={pj|j=1,‚Ä¶,m}\\\\boldsymbol{p}=\\\\{p_{j}|j=1,\\\\ldots,m\\\\}bold_italic_p = { italic_p start_POSTSUBSCRIPT italic_j end_POSTSUBSCRIPT | italic_j = 1, ‚Ä¶, italic_m } by mapping each xix_{i}italic_x start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT to the set {0,1} where 1 indicates the start of a new patch.\\nFor both token-based and patch-based models, the computational cost of processing data is primarily determined by the number of steps executed by the main Transformer. In BLT, this is the number of patches needed to encode the data with a given patching function.\\nConsequently, the average size of a patch, or simply patch size, is the main factor for determining the cost of processing data during both training and inference with a given patching function\\xa0(¬ß4.5).\\nNext, we introduce three patching functions: patching with a fixed number of bytes per patch\\xa0(¬ß2<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n', params: SamplingParams(n=1, presence_penalty=0.0, frequency_penalty=0.0, repetition_penalty=1.0, temperature=0.7, top_p=0.95, top_k=0, min_p=0.0, seed=None, stop=[], stop_token_ids=[], bad_words=[], include_stop_str_in_output=False, ignore_eos=False, max_tokens=512, min_tokens=0, logprobs=None, prompt_logprobs=None, skip_special_tokens=True, spaces_between_special_tokens=True, truncate_prompt_tokens=None, guided_decoding=None, extra_args=None), prompt_token_ids: None, prompt_embeds shape: None, lora_request: None, prompt_adapter_request: None.\n",
            "vLLM STDOUT: INFO 11-10 03:37:08 [engine.py:317] Added request chatcmpl-6498cd60203346659477dceba3210222.\n",
            "\u001b[2KProcessing 1 chunks to generate QA pairs...\n",
            "\u001b[2K\u001b[32m‚†ß\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...vLLM STDOUT: INFO:     127.0.0.1:60692 - \"POST /v1/chat/completions HTTP/1.1\" 200 OK\n",
            "\u001b[2KBatch processing complete.\n",
            "\u001b[2KGenerated 9 QA pairs total\n",
            "\u001b[2KSaving result to data/generated/arxiv_org_2_qa_pairs.json\n",
            "\u001b[2KSuccessfully wrote test file to data/generated/test_write.json\n",
            "\u001b[2KSuccessfully wrote result to data/generated/arxiv_org_2_qa_pairs.json\n",
            "\u001b[2K\u001b[32m‚†ã\u001b[0m Generating qa content from data/output/arxiv_org_2.txt...\n",
            "\u001b[1A\u001b[2K\u001b[32m Content saved to \u001b[0m\u001b[1;32mdata/generated/arxiv_org_2_qa_pairs.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "# Process 3 chunks for now -> can increase but slower!\n",
        "for filename in filenames[:3]:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        create {filename} \\\n",
        "        --num-pairs 25 \\\n",
        "        --type \"qa\"\n",
        "    time.sleep(2) # Sleep some time to leave some room for processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BRwGQLq98Jn"
      },
      "source": [
        "We now convert the generated datasets into QA formats so we can load it for finetuning:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVADsJTf9s2u",
        "outputId": "60d4ff46-28f9-467b-caff-ceca19478df1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/arxiv_org_0_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_0_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/arxiv_org_1_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_1_qa_pairs_ft.json\u001b[0m\n",
            "\u001b[?25l\u001b[32m‚†ã\u001b[0m Converting data/generated/arxiv_org_2_qa_pairs.json to ft format with json \n",
            "storage...\n",
            "\u001b[1A\u001b[2K\u001b[1A\u001b[2K\u001b[32m Converted to ft format and saved to \u001b[0m\u001b[1;32mdata/final/arxiv_org_2_qa_pairs_ft.json\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "qa_pairs_filenames = [\n",
        "    f\"data/generated/arxiv_org_{i}_qa_pairs.json\"\n",
        "    for i in range(len(filenames[:3]))\n",
        "]\n",
        "for filename in qa_pairs_filenames:\n",
        "    !synthetic-data-kit \\\n",
        "        -c synthetic_data_kit_config.yaml \\\n",
        "        save-as {filename} -f ft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dVK-qza7rPB"
      },
      "source": [
        "Let's load up the data and see what the synthetic data looks like!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VrBwG2KT7dam"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "final_filenames = [\n",
        "    f\"data/final/arxiv_org_{i}_qa_pairs_ft.json\"\n",
        "    for i in range(len(filenames[:3]))\n",
        "]\n",
        "conversations = pd.concat([\n",
        "    pd.read_json(name) for name in final_filenames\n",
        "]).reset_index(drop = True)\n",
        "\n",
        "dataset = Dataset.from_pandas(conversations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHyWyJGD-JCo",
        "outputId": "1bde1ac7-e638-443a-a0d4-214c1ab22a31"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'How does BLT models differ from MegaByte models in terms of patching?',\n",
              "   'role': 'user'},\n",
              "  {'content': 'BLT models group bytes into patches by striding every four bytes, unlike MegaByte models.',\n",
              "   'role': 'assistant'}]}"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aO9qePmP7yaY"
      },
      "source": [
        "Finally free vLLM process to save memory and to allow for finetuning!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F8qgTjywzgl6",
        "outputId": "95a637a3-a83f-490e-fcc5-09061b702094"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to terminate the VLLM server gracefully...\n",
            "vLLM STDOUT: INFO 11-10 03:37:27 [launcher.py:80] Shutting down FastAPI HTTP server.\n",
            "Server terminated gracefully.\n"
          ]
        }
      ],
      "source": [
        "generator.cleanup()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQo2PR7oqDQE"
      },
      "source": [
        "### Fine-tuning Synthetic Dataset with Unsloth"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUBVEnvCDJv",
        "outputId": "f4468467-c04f-4139-b5e9-02b0bd998ccd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth 2025.11.2: Fast Llama patching. Transformers: 4.56.2. vLLM: 0.9.2.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.2.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth: Float16 full finetuning uses more memory since we upcast weights to float32.\n",
            "HuggingFaceTB/SmolLM2-135M does not have a padding token! Will use pad_token = <|endoftext|>.\n"
          ]
        }
      ],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from google.colab import userdata\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "    # Qwen3 new models\n",
        "    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n",
        "    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n",
        "    # Other very popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"HuggingFaceTB/SmolLM2-135M\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = False,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = True, # [NEW!] We have full finetuning now!\n",
        "    token = userdata.get('HUGGING_FACE_TOKEN'), # use one if using gated models\n",
        "    torch_dtype=torch.float16,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vITh0KVJ10qX"
      },
      "source": [
        "<a name=\"Data\"></a>\n",
        "### Data Prep\n",
        "We now use the `Llama-3.2` format for conversation style finetunes. The chat template renders conversations like below: (Cutting Knowledge Date is by default there!)\n",
        "\n",
        "```\n",
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "\n",
        "Cutting Knowledge Date: December 2023\n",
        "Today Date: 01 May 2025\n",
        "\n",
        "You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is 1+1?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "2<|eot_id|>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cZoFztyDAMTk",
        "outputId": "5e67bbf1-3a36-4f0a-ba47-7d05341131e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model does not have a padding token! Will use pad_token = <|endoftext|>.\n"
          ]
        }
      ],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"llama-3.1\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "3ed06dfec62346fd964113ed93e430ab",
            "ce10de8df10e43c180edf1ee5f1d2c3f",
            "eccb1a16a2104f388f2872fa2ad166b1",
            "bdd232c10e5a4728b11b698efc6a0628",
            "ba4f0ea9a30841f7a021edcbb4eb2165",
            "250dcd1f159e4178938f8110fc77bdfa",
            "6d32e613d16141aa9f7df3d2b66060ae",
            "c0f2097596934728a90bdcad8e111582",
            "296b820f59ba4a8f91fed353fa97fe3e",
            "2b6ee424dcfc4d99932368b3e5beb942",
            "c9cc65bb71234838941aa89c61553633"
          ]
        },
        "id": "LjY75GoYUCB8",
        "outputId": "1151b98d-f281-470d-e42e-55b7859781b4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3ed06dfec62346fd964113ed93e430ab",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/32 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "def formatting_prompts_func(examples):\n",
        "    convos = examples[\"messages\"]\n",
        "    texts = [tokenizer.apply_chat_template(convo, tokenize = False, add_generation_prompt = False) for convo in convos]\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "# Get our previous dataset and format it:\n",
        "dataset = dataset.map(formatting_prompts_func, batched = True,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKA0VEF4CfCB"
      },
      "source": [
        "See result of the first row:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0usAI0M40hpT",
        "outputId": "4aa51de6-40be-4da0-f775-9a2911a9c983"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'messages': [{'content': 'You are a helpful assistant.', 'role': 'system'},\n",
              "  {'content': 'What is the main contribution of the Byte Latent Transformer (BLT) architecture?',\n",
              "   'role': 'user'},\n",
              "  {'content': 'The BLT architecture introduces a new byte-level LLM that matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.',\n",
              "   'role': 'assistant'}],\n",
              " 'text': '<|endoftext|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 26 July 2024\\n\\nYou are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the main contribution of the Byte Latent Transformer (BLT) architecture?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe BLT architecture introduces a new byte-level LLM that matches tokenization-based LLM performance at scale with significant improvements in inference efficiency and robustness.<|eot_id|>'}"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idAEIeSQ3xdS"
      },
      "source": [
        "<a name=\"Train\"></a>\n",
        "### Train the model\n",
        "Now let's train our model. We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 61,
          "referenced_widgets": [
            "57ba49f515a44b9682b49508b44971e5",
            "229553560a5b4a58be445187e5aedae5",
            "cf1f15fa6583488b84bd7dc3b43845e5",
            "1fe4bf7d34964dca8558061653ec00a3",
            "a3812b7cbe6d44c9bdf060da5986ad78",
            "3643c027f5ec467e8663b457f6fdc009",
            "4acc36b373524fdfbcbbe66836c5765a",
            "7280a5aed8624b17abb3b5225ee24299",
            "a11d2eeb9816445da9af54172e251bde",
            "dd74fc521ba94714bf960969471bb4ac",
            "528af071c70a4f72b9f16d36dd4e6572"
          ]
        },
        "id": "95_Nn-89DhsL",
        "outputId": "763852c8-5094-41f4-ea9d-81bc082c4668"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57ba49f515a44b9682b49508b44971e5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=12):   0%|          | 0/32 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 2,\n",
        "        gradient_accumulation_steps = 4, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        max_steps = 30,\n",
        "        learning_rate = 2e-4,\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.001,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use TrackIO/WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ejIt2xSNKKp",
        "outputId": "36d47aca-de2a-411b-8524-c94646a65d0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU = Tesla T4. Max memory = 14.741 GB.\n",
            "2.385 GB of memory reserved.\n"
          ]
        }
      ],
      "source": [
        "# @title Show current memory stats\n",
        "gpu_stats = torch.cuda.get_device_properties(0)\n",
        "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
        "print(f\"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.\")\n",
        "print(f\"{start_gpu_memory} GB of memory reserved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yqxqAZ7KJ4oL",
        "outputId": "0d3addde-7bcb-4035-ff7b-fbff335305f8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 32 | Num Epochs = 8 | Total steps = 30\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 134,515,008 of 134,515,008 (100.00% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='30' max='30' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [30/30 00:26, Epoch 7/8]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.386600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.367900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.747100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.282900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.024600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.829300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.657200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.471400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.487200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>0.411700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>0.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.332900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.257300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>0.257600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>0.233900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.196300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.176300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>0.161000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.155800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.118900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.113300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.113500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.103600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>0.109600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.080900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.072200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.069900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.058300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.067600</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCqnaKmlO1U9",
        "outputId": "b9456d7a-a8ac-49f2-f0a4-f930c41027ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28.0194 seconds used for training.\n",
            "0.47 minutes used for training.\n",
            "Peak reserved memory = 2.385 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 16.179 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNjoscj3BmuK",
        "outputId": "28fe671a-8cd1-46e3-f39c-4d472285c267"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "28.0194 seconds used for training.\n",
            "0.47 minutes used for training.\n",
            "Peak reserved memory = 2.385 GB.\n",
            "Peak reserved memory for training = 0.0 GB.\n",
            "Peak reserved memory % of max memory = 16.179 %.\n",
            "Peak reserved memory for training % of max memory = 0.0 %.\n"
          ]
        }
      ],
      "source": [
        "# @title Show final memory and time stats\n",
        "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
        "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
        "used_percentage = round(used_memory / max_memory * 100, 3)\n",
        "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
        "print(f\"{trainer_stats.metrics['train_runtime']} seconds used for training.\")\n",
        "print(\n",
        "    f\"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training.\"\n",
        ")\n",
        "print(f\"Peak reserved memory = {used_memory} GB.\")\n",
        "print(f\"Peak reserved memory for training = {used_memory_for_lora} GB.\")\n",
        "print(f\"Peak reserved memory % of max memory = {used_percentage} %.\")\n",
        "print(f\"Peak reserved memory for training % of max memory = {lora_percentage} %.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ekOmTR1hSNcr"
      },
      "source": [
        "<a name=\"Inference\"></a>\n",
        "### Inference\n",
        "Let's run the model! Use `apply_chat_template` with `add_generation_prompt` set to `True` for inference."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kR3gIAX-SM2q",
        "outputId": "76e23150-16b3-4ef9-9cf8-8c494bc0f1ad"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A novel architecture that uses patches to encode bytes<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What is the Byte Latent Transformer?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRrr--20Udm9"
      },
      "source": [
        "The model learns about the research paper!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2strt31SUc5W",
        "outputId": "3359dc6a-f533-4960-a875-400af205e20d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Improved inference capabilities, enhanced model robustness, and the ability to scale while maintaining a fixed-inference budget<|eot_id|>\n"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"What are some benefits of the BLT?\"},\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize = True,\n",
        "    add_generation_prompt = True, # Must add for generation\n",
        "    return_tensors = \"pt\",\n",
        ").to(\"cuda\")\n",
        "\n",
        "from transformers import TextStreamer\n",
        "text_streamer = TextStreamer(tokenizer, skip_prompt = True)\n",
        "_ = model.generate(input_ids = inputs, streamer = text_streamer,\n",
        "                   max_new_tokens = 256, temperature = 0.1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uMuVrWbjAzhc"
      },
      "source": [
        "<a name=\"Save\"></a>\n",
        "### Saving, loading finetuned models\n",
        "To save the final model as LoRA adapters, either use Huggingface's `push_to_hub` for an online save or `save_pretrained` for a local save.\n",
        "\n",
        "**[NOTE]** This ONLY saves the LoRA adapters, and not the full model. To save to 16bit or GGUF, scroll down!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upcOlWe7A1vc",
        "outputId": "b62c40e8-1e58-4d2b-f604-8a36704b5c4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('fine_tuned_llama_3.2/tokenizer_config.json',\n",
              " 'fine_tuned_llama_3.2/special_tokens_map.json',\n",
              " 'fine_tuned_llama_3.2/chat_template.jinja',\n",
              " 'fine_tuned_llama_3.2/vocab.json',\n",
              " 'fine_tuned_llama_3.2/merges.txt',\n",
              " 'fine_tuned_llama_3.2/added_tokens.json',\n",
              " 'fine_tuned_llama_3.2/tokenizer.json')"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.save_pretrained(\"fine_tuned_llama_3.2\")  # Local saving\n",
        "tokenizer.save_pretrained(\"fine_tuned_llama_3.2\")\n",
        "# model.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving\n",
        "# tokenizer.push_to_hub(\"your_name/lora_model\", token = \"...\") # Online saving"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "authorship_tag": "ABX9TyPYx/7baYyI0DLG/4vdmPTl",
      "gpuType": "T4",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
