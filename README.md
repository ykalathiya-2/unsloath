# Modern AI with Unsloth.ai

## Course Information
- **Course**: CMPE-255 Data Mining (Section 47)
- **Semester**: Fall 2025
- **Due Date**: November 9, 2024, 11:59 PM

---

## Assignment Overview
This repository contains 5 colab notebooks demonstrating different LLM fine-tuning techniques using unsloth.ai:

1. **Full Fine-tuning** - Complete model parameter updates
2. **LoRA Fine-tuning** - Parameter efficient training
3. **RLHF (DPO)** - Reinforcement learning with human feedback
4. **GRPO** - Reasoning model training
5. **Continued Pre-training** - Teaching new languages/domains

---

## üìì Colab Notebooks

### Colab 1: Full Fine-tuning
- **File**: `unsloath_full_finetuning.ipynb`
- **Model**: SmolLM2-135M
- **Method**: Full fine-tuning (all parameters updated)
- **Dataset**: Synthetic QA pairs from research paper
- **YouTube**: [Video Link](https://drive.google.com/file/d/1XDPS5i-p6zmkCDvV88WtxZ9rXgijykU2/view?usp=drive_link)

### Colab 2: LoRA Fine-tuning
- **File**: `colab2_lora_finetuning.ipynb`
- **Model**: SmolLM2-135M
- **Method**: LoRA (Parameter Efficient Fine-tuning)
- **Dataset**: Same as Colab 1
- **YouTube**: [Video Link](https://drive.google.com/file/d/1fpW_0uVxAuC7301CwlzBBvbKGJ0vPVwK/view?usp=drive_link)

### Colab 3: RLHF with DPO
- **File**: `colab3_rlhf.ipynb`
- **Model**: Llama-3.2-3B
- **Method**: Direct Preference Optimization
- **Dataset**: Preferred and rejected responses
- **YouTube**: [Video Link](https://drive.google.com/file/d/1xzk92Q5QEri0Hn1za_7ymWDZvt1EYbLF/view?usp=drive_link)

### Colab 4: GRPO Reasoning
- **File**: `colab4_grpo_reasoning.ipynb`
- **Model**: Llama-3.2-3B
- **Method**: Group Relative Policy Optimization
- **Dataset**: Math/coding problems
- **YouTube**: [Video Link](https://drive.google.com/file/d/1bXKgZCWig7iggAa7EqkjjA1iNvYxmuTs/view?usp=drive_link)

### Colab 5: Continued Pre-training
- **File**: `colab5_continued_pretraining.ipynb`
- **Model**: Base LLM
- **Method**: Continued pre-training
- **Dataset**: New language/domain corpus
- **YouTube**: [Video Link](https://drive.google.com/file/d/1u98b1Cw8SJed1maf8yzDENz51m6A99ke/view?usp=drive_link)

---

## üöÄ Quick Setup

### Installation
```bash
pip install --upgrade uv
pip install unsloth vllm synthetic-data-kit==0.0.3
uv pip install transformers==4.56.2
uv pip install --no-deps trl==0.22.2
```

### GPU Requirements
- Full Fine-tuning: 8GB+ VRAM
- LoRA: 4GB+ VRAM
- RLHF/GRPO: 8GB+ VRAM

---

## üìÅ Project Files
```
unsloath/
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ unsloath_full_finetuning.ipynb
‚îú‚îÄ‚îÄ colab2_lora_finetuning.ipynb
‚îú‚îÄ‚îÄ colab3_rlhf.ipynb
‚îú‚îÄ‚îÄ colab4_grpo_reasoning.ipynb
‚îî‚îÄ‚îÄ colab5_continued_pretraining.ipynb
```

---

## üîë Key Concepts

**Full Fine-tuning vs LoRA**
- Full: Updates all parameters, more accurate, slower
- LoRA: Only trains adapters, 90% less memory, faster

**RLHF vs GRPO**
- RLHF (DPO): Uses labeled preferred/rejected pairs
- GRPO: Model generates and evaluates its own outputs

---

## üìö References

**Official Documentation**
- [Unsloth Documentation](https://docs.unsloth.ai/)
- [Fine-tuning Guide](https://docs.unsloth.ai/get-started/fine-tuning-llms-guide)
- [RL Guide](https://docs.unsloth.ai/get-started/reinforcement-learning-rl-guide)
- [Continued Pre-training](https://docs.unsloth.ai/basics/continued-pretraining)

**GitHub & Tutorials**
- [Unsloth Notebooks](https://github.com/unslothai/notebooks/)
- [Kaggle Tutorial](https://www.kaggle.com/code/kingabzpro/fine-tuning-llms-using-unsloth)
- [Ollama Export Guide](https://docs.unsloth.ai/tutorials/how-to-finetune-llama-3-and-export-to-ollama)

